{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a93c9b-c71b-4909-b23d-f406c96f08f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5914244-0e58-4edc-9b03-1428cb92e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import config\n",
    "import MODEL\n",
    "import UTILS\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b216de-e01f-4417-986a-71c566f84af2",
   "metadata": {},
   "source": [
    "# HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31510b15-2cc5-439f-96df-afee5f82de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME SERIES INFO\n",
    "window = config.window\n",
    "stride = config.stride\n",
    "\n",
    "# CHANNELS INFO\n",
    "dynamic_channels = config.dynamic_channels\n",
    "static_channels = config.static_channels\n",
    "output_channels = config.output_channels\n",
    "\n",
    "# LABELS INFO\n",
    "unknown = config.unknown\n",
    "\n",
    "# MODEL INFO\n",
    "model_name = \"lstm\"\n",
    "code_dim = config.code_dim\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(config.device)\n",
    "\n",
    "# TRAIN INFO\n",
    "train = config.train\n",
    "batch_size = config.batch_size\n",
    "epochs = config.epochs\n",
    "learning_rate = config.learning_rate\n",
    "train_percent = config.train_percent\n",
    "\n",
    "print(\"Hyperparameters:{}\".format(model_name))\n",
    "print(\"window : {}\".format(window))\n",
    "print(\"stride : {}\".format(stride))\n",
    "print(\"dynamic_channels : {}\".format(dynamic_channels))\n",
    "print(\"static_channels : {}\".format(static_channels))\n",
    "print(\"output_channels : {}\".format(output_channels))\n",
    "print(\"unknown : {}\".format(unknown))\n",
    "print(\"model_name : {}\".format(model_name))\n",
    "print(\"code_dim : {}\".format(code_dim))\n",
    "print(\"device : {}\".format(device))\n",
    "print(\"train : {}\".format(train))\n",
    "print(\"batch_size : {}\".format(batch_size))\n",
    "print(\"epochs : {}\".format(epochs))\n",
    "print(\"learning_rate : {}\".format(learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92a74d4-af69-4b31-bbee-6017f0d061a4",
   "metadata": {},
   "source": [
    "# DEFINE DIRECTORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c124426-3f0a-4d5e-979d-25542795113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSED_DIR = config.PREPROCESSED_DIR\n",
    "RESULT_DIR = config.RESULT_DIR\n",
    "MODEL_DIR = config.MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b07a3e-1ca6-4182-adaa-a9adce86ecd1",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f51d0c-26d2-4c83-a1a8-7b4265f7e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file):\n",
    "\tdataset = np.load(os.path.join(PREPROCESSED_DIR, \"{}.npz\".format(file)), allow_pickle=True)\n",
    "\treturn dataset\n",
    "\n",
    "def get_data(dataset, index, preprocessed=True):\n",
    "\tdata = dataset[\"data\"]\n",
    "\tif preprocessed:\n",
    "\t\tdata = (data-dataset[\"train_data_means\"])/dataset[\"train_data_stds\"]\n",
    "\tdata = np.nan_to_num(data, nan=unknown)\n",
    "\tdata = data[dataset[index]]\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e4312a-bd8b-41ec-ab26-c141f1bb4b9b",
   "metadata": {},
   "source": [
    "# BUILD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b98f3d-d227-474b-a28c-d0c86a790f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getattr(MODEL, model_name)(input_channels=len(dynamic_channels), code_dim=code_dim, output_channels=len(output_channels))\n",
    "model = model.to(device)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"#Parameters:{}\".format(pytorch_total_params))\n",
    "print(model)\n",
    "criterion = torch.nn.MSELoss(reduction=\"none\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db10870-972b-4882-b212-0dad8e4ce622",
   "metadata": {},
   "source": [
    "# TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe0511-6f34-4e19-aad6-57511b0f1e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "\n",
    "\ttrain_loss = []\n",
    "\tvalid_loss = []\n",
    "\tmin_loss = 10000\n",
    "\n",
    "\tfor epoch in range(1,epochs+1):\n",
    "\n",
    "\t\t# LOSS ON TRAIN SET\n",
    "\t\tmodel.train()\n",
    "\n",
    "\t\t# LOAD DATA\n",
    "\t\tfile, index = \"strided_train\", \"train_index\"\n",
    "\t\tdataset = load_dataset(file)\n",
    "\t\tdata = get_data(dataset, index)\n",
    "\t\tnodes, years, window, channels = data.shape\n",
    "\t\t# print(nodes, years, window, channels)\n",
    "\n",
    "\t\t# GET RANDOM YEARS\n",
    "\t\trandom_years = np.zeros((nodes, years))\n",
    "\t\tfor node in range(nodes):\n",
    "\t\t\trandom_years[node] = random.sample(range(years), years)\n",
    "\t\trandom_years = random_years.astype(np.int64)\n",
    "\t\t# print(random_years.shape)\n",
    "\n",
    "\t\t# LOSS\n",
    "\t\tepoch_loss = 0\n",
    "\t\tfor year in range(random_years.shape[1]):\n",
    "\n",
    "\t\t\t#Get instance for each node\n",
    "\t\t\tnode_data = data[np.arange(nodes), random_years[:, year]]\n",
    "\t\t\t# print(node_data.shape)\n",
    "\n",
    "\t\t\trandom_batches = random.sample(range(node_data.shape[0]),node_data.shape[0])\n",
    "\t\t\tfor batch in range(math.ceil(nodes/batch_size)):\n",
    "\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t\t# GET BATCH DATA AND LABEL\n",
    "\t\t\t\trandom_batch = random_batches[batch*batch_size:(batch+1)*batch_size]\n",
    "\t\t\t\tbatch_data = torch.from_numpy(node_data[random_batch]).to(device)\n",
    "\t\t\t\tbatch_dynamic_input = batch_data[:, :, dynamic_channels].to(device)\n",
    "\t\t\t\tbatch_label = batch_data[:, :, output_channels].to(device)\n",
    "\t\t\t\t# print(batch_input.shape, batch_label.shape)\n",
    "\n",
    "\t\t\t\t# GET OUTPUT\n",
    "\t\t\t\tbatch_pred = model(x_dynamic=batch_dynamic_input)\n",
    "\t\t\t\t# print(batch_pred.shape)\n",
    "\n",
    "\t\t\t\t# CALCULATE LOSS\n",
    "\t\t\t\tbatch_loss = criterion(batch_label, batch_pred)\t\t\t\t\t\t\t\t\t\t\t# PER CHANNEL LOSS\n",
    "\t\t\t\tmask = (batch_label!=unknown).float()\t\t\t\t\t\t\t\t\t\t\t\t\t# CREATE MASK\n",
    "\t\t\t\tbatch_loss = batch_loss * mask\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# MULTIPLY MASK\n",
    "\t\t\t\tbatch_loss, mask = torch.sum(batch_loss, dim=2), (torch.sum(mask, dim=2)>0).float()\t\t# PER INSTANCE LOSS\n",
    "\t\t\t\tbatch_loss = torch.sum(batch_loss)/torch.sum(mask)\t\t\t\t\t\t\t\t\t\t# MEAN SEQUENCE LOSS\n",
    "\t\t\t\t# print(batch_loss.shape)\n",
    "\n",
    "\t\t\t\t# LOSS BACKPROPOGATE\n",
    "\t\t\t\tbatch_loss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t\t# AGGREGATE LOSS\n",
    "\t\t\t\tepoch_loss += batch_loss.item()\n",
    "\n",
    "\t\tepoch_loss /= ((batch+1)*(year+1))\n",
    "\t\tprint('Epoch:{}\\tTrain Loss:{:.4f}'.format(epoch, epoch_loss), end=\"\\t\")\n",
    "\t\ttrain_loss.append(epoch_loss)\n",
    "\n",
    "\t\t# SCORE ON VALIDATION SET\n",
    "\t\tmodel.eval()\n",
    "\n",
    "\t\t# LOAD DATA\n",
    "\t\tfile, index = \"strided_valid\", \"train_index\"\n",
    "\t\tdataset = load_dataset(file)\n",
    "\t\tdata = get_data(dataset, index)\n",
    "\t\tnodes, years, window, channels = data.shape\n",
    "\t\t# print(nodes, years, window, channels)\n",
    "\n",
    "\t\t# SCORE\n",
    "\t\tepoch_loss = 0\n",
    "\t\tfor year in range(years):\n",
    "\n",
    "\t\t\t#Get instance for each node\n",
    "\t\t\tnode_data = data[np.arange(nodes), year]\n",
    "\t\t\t# print(node_data.shape)\n",
    "\n",
    "\t\t\tfor batch in range(math.ceil(nodes/batch_size)):\n",
    "\n",
    "\t\t\t\t# GET BATCH DATA AND LABEL\n",
    "\t\t\t\tbatch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\t\t\tbatch_dynamic_input = batch_data[:, :, dynamic_channels].to(device)\n",
    "\t\t\t\tbatch_label = batch_data[:, :, output_channels].to(device)\n",
    "\t\t\t\t# print(batch_input.shape, batch_label.shape)\n",
    "\n",
    "\t\t\t\t# GET OUTPUT\n",
    "\t\t\t\tbatch_pred = model(x_dynamic=batch_dynamic_input)\n",
    "\t\t\t\t# print(batch_pred.shape)\n",
    "\n",
    "\t\t\t\t# CALCULATE LOSS\n",
    "\t\t\t\tbatch_loss = criterion(batch_label, batch_pred)\t\t\t\t\t\t\t\t\t\t\t# PER CHANNEL LOSS\n",
    "\t\t\t\tmask = (batch_label!=unknown).float()\t\t\t\t\t\t\t\t\t\t\t\t\t# CREATE MASK\n",
    "\t\t\t\tbatch_loss = batch_loss * mask\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# MULTIPLY MASK\n",
    "\t\t\t\tbatch_loss, mask = torch.sum(batch_loss, dim=2), (torch.sum(mask, dim=2)>0).float()\t\t# PER SEQUENCE LOSS\n",
    "\t\t\t\tbatch_loss = torch.sum(batch_loss)/torch.sum(mask)\t\t\t\t\t\t\t\t\t\t# MEAN SEQUENCE LOSS\n",
    "\t\t\t\t# print(batch_loss.shape)\n",
    "\n",
    "\t\t\t\t# AGGREGATE LOSS\n",
    "\t\t\t\tepoch_loss += batch_loss.item()\n",
    "\n",
    "\t\tepoch_loss /= ((batch+1)*(year+1))\n",
    "\t\tprint(\"Val Loss:{:.4f}\\tMin Loss:{:.4f}\".format(epoch_loss, min_loss))\n",
    "\t\tvalid_loss.append(epoch_loss)\n",
    "\t\tif min_loss>epoch_loss:\n",
    "\t\t\tmin_loss = epoch_loss\n",
    "\t\t\ttorch.save(model.state_dict(), os.path.join(MODEL_DIR, model_name))\n",
    "\n",
    "\t# PLOT LOSS\n",
    "\tfig = plt.figure(figsize=(10,10))\n",
    "\tax1 = fig.add_subplot(111)\n",
    "\tax1.set_xlabel(\"#Epoch\", fontsize=50)\n",
    "\n",
    "\t# PLOT TRAIN LOSS\n",
    "\tlns1 = ax1.plot(train_loss, color='red', marker='o', linewidth=4, label=\"TRAIN LOSS\")\n",
    "\n",
    "\t# PLOT VALIDATION SCORE\n",
    "\tax2 = ax1.twinx()\n",
    "\tlns2 = ax2.plot(valid_loss, color='blue', marker='o', linewidth=4, label=\"VAL LOSS\")\n",
    "\n",
    "\t# added these three lines\n",
    "\tlns = lns1+lns2\n",
    "\tlabs = [l.get_label() for l in lns]\n",
    "\tax1.legend(lns, labs, loc=\"upper right\", fontsize=40, frameon=False)\n",
    "\n",
    "\tplt.tight_layout(pad=0.0,h_pad=0.0,w_pad=0.0)\n",
    "\tplt.savefig(os.path.join(RESULT_DIR, \"{}_SCORE.pdf\".format(model_name)), format = \"pdf\")\n",
    "\tplt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eccbe1d-d7b2-40ef-8c87-4c80deb61748",
   "metadata": {},
   "source": [
    "# LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c21078f-ed88-4f18-9d6c-85afa3ac59b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(MODEL_DIR, model_name)))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36dfda6-f28d-4494-8c39-f63c08094838",
   "metadata": {},
   "source": [
    "# TEST MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97e25d-0068-46ac-afd8-32cb452a7621",
   "metadata": {},
   "source": [
    "## IN DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e587aba7-25b1-4c56-9b05-26251338d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file, index = \"strided_test\", \"train_index\"\n",
    "dataset = load_dataset(file)\n",
    "data = get_data(dataset, index)\n",
    "nodes, years, window, channels = data.shape\n",
    "# print(nodes, years, window, channels)\n",
    "\n",
    "dataset_true = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "dataset_pred = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "for year in range(years):\n",
    "\n",
    "\t#Get instance for each node\n",
    "\tnode_data = data[np.arange(nodes), year]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\tfor batch in range(math.ceil(nodes/batch_size)):\n",
    "\n",
    "\t\t# GET BATCH DATA AND LABEL\n",
    "\t\tbatch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_dynamic_input = batch_data[:, :, dynamic_channels].to(device)\n",
    "\t\tbatch_label = batch_data[:, :, output_channels].to(device)\n",
    "\t\t# print(batch_input.shape, batch_label.shape)\n",
    "\n",
    "\t\t# GET OUTPUT\n",
    "\t\tbatch_pred = model(x_dynamic=batch_dynamic_input)\n",
    "\t\t# print(batch_pred.shape)\n",
    "\n",
    "\t\t# STORE OUTPUT\n",
    "\t\tdataset_true[batch*batch_size:(batch+1)*batch_size, year] = batch_label.detach().cpu().numpy()\n",
    "\t\tdataset_pred[batch*batch_size:(batch+1)*batch_size, year] = batch_pred.detach().cpu().numpy()\n",
    "\n",
    "dataset_true = (dataset_true*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "dataset_pred = (dataset_pred*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "dataset_true = UTILS.unstride_array(dataset_true)\n",
    "dataset_pred = UTILS.unstride_array(dataset_pred)\n",
    "dataset_true = dataset_true[:, stride:]\n",
    "dataset_pred = dataset_pred[:, stride:]\n",
    "\n",
    "per_sample_RMSE = UTILS.per_sample_RMSE(dataset_true, dataset_pred, unknown)\n",
    "_, per_node_RMSE = UTILS.per_node_RMSE(dataset_true, dataset_pred, unknown)\n",
    "per_sample_R2 = UTILS.per_sample_R2(dataset_true, dataset_pred, unknown)\n",
    "_, per_node_R2 = UTILS.per_node_R2(dataset_true, dataset_pred, unknown)\n",
    "print(\"Per Sample RMSE:{:.4f}\\tPer Node RMSE:{:.4f}\\tPer Sample R2:{:.4f}\\tPer Node R2:{:.4f}\".format(per_sample_RMSE, per_node_RMSE, per_sample_R2, per_node_R2))\n",
    "np.save(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"true\")), dataset_true)\n",
    "np.save(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, model_name)), dataset_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e84e69-c720-4250-8076-1d4b7d07db71",
   "metadata": {},
   "source": [
    "## OUT DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545cbf08-6127-4b4b-a010-1cf6e0e6679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file, index = \"strided_test\", \"test_index\"\n",
    "dataset = load_dataset(file)\n",
    "data = get_data(dataset, index)\n",
    "nodes, years, window, channels = data.shape\n",
    "# print(nodes, years, window, channels)\n",
    "\n",
    "dataset_true = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "dataset_pred = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "for year in range(years):\n",
    "\n",
    "\t#Get instance for each node\n",
    "\tnode_data = data[np.arange(nodes), year]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\tfor batch in range(math.ceil(nodes/batch_size)):\n",
    "\n",
    "\t\t# GET BATCH DATA AND LABEL\n",
    "\t\tbatch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_dynamic_input = batch_data[:, :, dynamic_channels].to(device)\n",
    "\t\tbatch_label = batch_data[:, :, output_channels].to(device)\n",
    "\t\t# print(batch_input.shape, batch_label.shape)\n",
    "\n",
    "\t\t# GET OUTPUT\n",
    "\t\tbatch_pred = model(x_dynamic=batch_dynamic_input)\n",
    "\t\t# print(batch_pred.shape)\n",
    "\n",
    "\t\t# STORE OUTPUT\n",
    "\t\tdataset_true[batch*batch_size:(batch+1)*batch_size, year] = batch_label.detach().cpu().numpy()\n",
    "\t\tdataset_pred[batch*batch_size:(batch+1)*batch_size, year] = batch_pred.detach().cpu().numpy()\n",
    "\n",
    "dataset_true = (dataset_true*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "dataset_pred = (dataset_pred*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "dataset_true = UTILS.unstride_array(dataset_true)\n",
    "dataset_pred = UTILS.unstride_array(dataset_pred)\n",
    "dataset_true = dataset_true[:, stride:]\n",
    "dataset_pred = dataset_pred[:, stride:]\n",
    "\n",
    "per_sample_RMSE = UTILS.per_sample_RMSE(dataset_true, dataset_pred, unknown)\n",
    "_, per_node_RMSE = UTILS.per_node_RMSE(dataset_true, dataset_pred, unknown)\n",
    "per_sample_R2 = UTILS.per_sample_R2(dataset_true, dataset_pred, unknown)\n",
    "_, per_node_R2 = UTILS.per_node_R2(dataset_true, dataset_pred, unknown)\n",
    "print(\"Per Sample RMSE:{:.4f}\\tPer Node RMSE:{:.4f}\\tPer Sample R2:{:.4f}\\tPer Node R2:{:.4f}\".format(per_sample_RMSE, per_node_RMSE, per_sample_R2, per_node_R2))\n",
    "np.save(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"true\")), dataset_true)\n",
    "np.save(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, model_name)), dataset_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
