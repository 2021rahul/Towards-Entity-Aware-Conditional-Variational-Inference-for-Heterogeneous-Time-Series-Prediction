{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import config\n",
    "import MODEL\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME SERIES INFO\n",
    "window = config.window\n",
    "\n",
    "# CHANNELS INFO\n",
    "dynamic_channels = config.dynamic_channels\n",
    "static_channels = config.static_channels\n",
    "output_channels = config.output_channels\n",
    "\n",
    "# LABELS INFO\n",
    "unknown = config.unknown\n",
    "\n",
    "# MODEL INFO\n",
    "model_name = \"kgssl\"\n",
    "code_dim = config.code_dim\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(config.device)\n",
    "recon_weight = config.recon_weight\n",
    "contrastive_weight = config.contrastive_weight\n",
    "static_weight = config.static_weight\n",
    "sum_weight = recon_weight+contrastive_weight+static_weight\n",
    "\n",
    "# TRAIN INFO\n",
    "train = config.train\n",
    "batch_size = config.batch_size\n",
    "epochs = config.epochs\n",
    "learning_rate = config.learning_rate\n",
    "\n",
    "print(\"Hyperparameters:{}\".format(model_name))\n",
    "print(\"window : {}\".format(window))\n",
    "print(\"dynamic_channels : {}\".format(dynamic_channels))\n",
    "print(\"static_channels : {}\".format(static_channels))\n",
    "print(\"output_channels : {}\".format(output_channels))\n",
    "print(\"unknown : {}\".format(unknown))\n",
    "print(\"model_name : {}\".format(model_name))\n",
    "print(\"code_dim : {}\".format(code_dim))\n",
    "print(\"device : {}\".format(device))\n",
    "print(\"recon_weight : {}\".format(recon_weight))\n",
    "print(\"contrastive_weight : {}\".format(contrastive_weight))\n",
    "print(\"static_weight : {}\".format(static_weight))\n",
    "print(\"train : {}\".format(train))\n",
    "print(\"batch_size : {}\".format(batch_size))\n",
    "print(\"epochs : {}\".format(epochs))\n",
    "print(\"learning_rate : {}\".format(learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE DIRECTORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSED_DIR = config.PREPROCESSED_DIR\n",
    "RESULT_DIR = config.RESULT_DIR\n",
    "MODEL_DIR = config.MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file):\n",
    "\tdataset = np.load(os.path.join(PREPROCESSED_DIR, \"{}.npz\".format(file)), allow_pickle=True)\n",
    "\treturn dataset\n",
    "\n",
    "def get_data(dataset, index, preprocessed=True):\n",
    "\tdata = dataset[\"data\"]\n",
    "\tif preprocessed:\n",
    "\t\tdata = (data-dataset[\"train_data_means\"])/dataset[\"train_data_stds\"]\n",
    "\tdata = np.nan_to_num(data, nan=unknown)\n",
    "\tdata = data[dataset[index]]\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getattr(MODEL, \"ae\")(input_channels=len(dynamic_channels)+len(output_channels), code_dim=code_dim, output_channels=len(static_channels), device=device)\n",
    "model = model.to(device)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"#Parameters:{}\".format(pytorch_total_params))\n",
    "print(model)\n",
    "mse_criterion = torch.nn.MSELoss(reduction=\"none\")\n",
    "contrastive_criterion = MODEL.SimCLRLoss(temperature=5e-01)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "\n",
    "\ttrain_loss = []\n",
    "\tvalid_loss = []\n",
    "\tmin_loss = 10000\n",
    "\n",
    "\tfor epoch in range(1,epochs+1):\n",
    "\n",
    "\t\t# LOSS ON TRAIN SET\n",
    "\t\tmodel.train()\n",
    "\n",
    "\t\t# LOAD DATA\n",
    "\t\tfile, index = \"strided_train\", \"train_index\"\n",
    "\t\tdataset = load_dataset(file)\n",
    "\t\tdata = get_data(dataset, index)\n",
    "\t\tnodes, years, window, channels = data.shape\n",
    "\t\t# print(nodes, years, window, channels)\n",
    "\n",
    "\t\t# GET ANCHOR AND POSITIVE YEARS\n",
    "\t\tanchor_years = np.zeros((nodes, years))\n",
    "\t\tfor node in range(nodes):\n",
    "\t\t\tanchor_years[node] = random.sample(range(years), years)\n",
    "\t\tanchor_years = anchor_years.astype(np.int64)\n",
    "\t\tpositive_years = np.zeros((nodes, years))\n",
    "\t\tfor node in range(nodes):\n",
    "\t\t\tpositive_years[node] = random.sample(range(years), years)\n",
    "\t\tpositive_years = positive_years.astype(np.int64)\n",
    "\t\t# print(anchor_years.shape, positive_years.shape)\n",
    "\n",
    "\t\t# LOSS\n",
    "\t\tmodel.train()\n",
    "\t\tepoch_loss = 0\n",
    "\t\tepoch_recon_loss = 0\n",
    "\t\tepoch_contrastive_loss = 0\n",
    "\t\tepoch_static_loss = 0\n",
    "\t\tfor year in range(anchor_years.shape[1]):\n",
    "\n",
    "\t\t\t#Get (anchor,positive) Instances for each node\n",
    "\t\t\tanchor_data = data[np.arange(nodes), anchor_years[:, year]]\n",
    "\t\t\tpositive_data = data[np.arange(nodes), positive_years[:, year]]\n",
    "\t\t\t# print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "\t\t\t# Remove pairs where (anchor,positive) years are same\n",
    "\t\t\tkeep_idx = anchor_years[:, year] != positive_years[:, year]\n",
    "\t\t\tanchor_data = anchor_data[keep_idx]\n",
    "\t\t\tpositive_data = positive_data[keep_idx]\n",
    "\t\t\t# print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "\t\t\t# Remove pairs where (anchor,positive) basins have unknown in streamflow\n",
    "\t\t\tkeep_idx = np.zeros((anchor_data.shape[0], 2)).astype(bool)\n",
    "\t\t\tkeep_idx[:,0] = (anchor_data[:,:,-1]!=unknown).all(axis=1)\n",
    "\t\t\tkeep_idx[:,1] = (positive_data[:,:,-1]!=unknown).all(axis=1)\n",
    "\t\t\tkeep_idx = keep_idx.all(axis=1)\n",
    "\t\t\tanchor_data = anchor_data[keep_idx]\n",
    "\t\t\tpositive_data = positive_data[keep_idx]\n",
    "\t\t\t# print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "\t\t\trandom_batches = random.sample(range(anchor_data.shape[0]),anchor_data.shape[0])\n",
    "\t\t\tfor batch in range(math.ceil(anchor_data.shape[0]/batch_size)):\n",
    "\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t\t# GET BATCH DATA\n",
    "\t\t\t\trandom_batch = random_batches[batch*batch_size:(batch+1)*batch_size]\n",
    "\t\t\t\tbatch_anchor_data = torch.from_numpy(anchor_data[random_batch]).to(device)\n",
    "\t\t\t\tbatch_positive_data = torch.from_numpy(positive_data[random_batch]).to(device)\n",
    "\t\t\t\tbatch_input = torch.cat((batch_anchor_data[:,:,dynamic_channels+output_channels], batch_positive_data[:,:,dynamic_channels+output_channels]), dim=0)\n",
    "\t\t\t\tbatch_static = torch.cat((batch_anchor_data[:,0,static_channels], batch_positive_data[:,0,static_channels]), axis=0)\n",
    "\t\t\t\t# print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "\t\t\t\t# GET OUTPUT\n",
    "\t\t\t\tbatch_code_vec, batch_static_pred, batch_input_pred = model(x=batch_input)\n",
    "\t\t\t\t# print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "\t\t\t\t# CALCULATE LOSS\n",
    "\t\t\t\tbatch_recon_loss = mse_criterion(batch_input_pred, batch_input)\t\t\t\t\t\t\t\t# RECON LOSS\n",
    "\t\t\t\tbatch_recon_loss = torch.sum(batch_recon_loss, axis=2)\t\t\t\t\t\t\t\t\t\t# RECON LOSS\n",
    "\t\t\t\tbatch_recon_loss = torch.mean(batch_recon_loss)\t\t\t\t\t\t\t\t\t\t\t\t# RECON LOSS\n",
    "\t\t\t\tbatch_contrastive_loss = contrastive_criterion(batch_code_vec)\t\t\t\t\t\t\t\t# CONTRASTIVE LOSS\n",
    "\t\t\t\tbatch_static_loss = torch.mean(mse_criterion(batch_static_pred, batch_static), axis=1)\t\t# INVERSE LOSS\n",
    "\t\t\t\tbatch_static_loss = torch.mean(batch_static_loss)\t\t\t\t\t\t\t\t\t\t\t# INVERSE LOSS\n",
    "\t\t\t\tbatch_loss = (recon_weight*batch_recon_loss + contrastive_weight*batch_contrastive_loss + static_weight*batch_static_loss)/sum_weight\n",
    "\t\t\t\t# print(batch_loss.shape, batch_loss)\n",
    "\n",
    "\t\t\t\t# LOSS BACKPROPOGATE\n",
    "\t\t\t\tbatch_loss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t\t# AGGREGATE LOSS\n",
    "\t\t\t\tepoch_loss += batch_loss.item()\n",
    "\t\t\t\tepoch_recon_loss += batch_recon_loss.item()\n",
    "\t\t\t\tepoch_contrastive_loss += batch_contrastive_loss.item()\n",
    "\t\t\t\tepoch_static_loss += batch_static_loss.item()\n",
    "\n",
    "\t\tepoch_loss /= ((batch+1)*(year+1))\n",
    "\t\tepoch_recon_loss /= ((batch+1)*(year+1))\n",
    "\t\tepoch_contrastive_loss /= ((batch+1)*(year+1))\n",
    "\t\tepoch_static_loss /= ((batch+1)*(year+1))\n",
    "\t\tprint('Epoch:{}\\tTrain Loss:{:.4f}\\tRecon Loss:{:.4f}\\tCont Loss:{:.4f}\\tStatic Loss:{:.4f}'.format(epoch, epoch_loss, epoch_recon_loss, epoch_contrastive_loss, epoch_static_loss), end=\"\\t\")\n",
    "\t\ttrain_loss.append(epoch_loss)\n",
    "\n",
    "\t\t# LOSS ON VALIDATION SET\n",
    "\t\tmodel.eval()\n",
    "\n",
    "\t\t# LOAD DATA\n",
    "\t\tfile, index = \"strided_valid\", \"train_index\"\n",
    "\t\tdataset = load_dataset(file)\n",
    "\t\tdata = get_data(dataset, index)\n",
    "\t\tnodes, years, window, channels = data.shape\n",
    "\t\t# print(nodes, years, window, channels)\n",
    "\n",
    "\t\t# GET ANCHOR AND POSITIVE YEARS\n",
    "\t\tanchor_years = np.zeros((nodes, years))\n",
    "\t\tfor node in range(nodes):\n",
    "\t\t\tanchor_years[node] = random.sample(range(years), years)\n",
    "\t\tanchor_years = anchor_years.astype(np.int64)\n",
    "\t\tpositive_years = np.zeros((nodes, years))\n",
    "\t\tfor node in range(nodes):\n",
    "\t\t\tpositive_years[node] = random.sample(range(years), years)\n",
    "\t\tpositive_years = positive_years.astype(np.int64)\n",
    "\t\t# print(anchor_years.shape, positive_years.shape)\n",
    "\n",
    "\t\t# LOSS\n",
    "\t\tepoch_loss = 0\n",
    "\t\tepoch_recon_loss = 0\n",
    "\t\tepoch_contrastive_loss = 0\n",
    "\t\tepoch_static_loss = 0\n",
    "\t\tfor year in range(anchor_years.shape[1]):\n",
    "\n",
    "\t\t\t#Get (anchor,positive) Instances for each node\n",
    "\t\t\tanchor_data = data[np.arange(nodes), anchor_years[:, year]]\n",
    "\t\t\tpositive_data = data[np.arange(nodes), positive_years[:, year]]\n",
    "\t\t\t# print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "\t\t\t# Remove pairs where (anchor,positive) years are same\n",
    "\t\t\tkeep_idx = anchor_years[:, year] != positive_years[:, year]\n",
    "\t\t\tanchor_data = anchor_data[keep_idx]\n",
    "\t\t\tpositive_data = positive_data[keep_idx]\n",
    "\t\t\t# print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "\t\t\t# Remove pairs where (anchor,positive) basins have unknown in streamflow\n",
    "\t\t\tkeep_idx = np.zeros((anchor_data.shape[0], 2)).astype(bool)\n",
    "\t\t\tkeep_idx[:,0] = (anchor_data[:,:,-1]!=unknown).all(axis=1)\n",
    "\t\t\tkeep_idx[:,1] = (positive_data[:,:,-1]!=unknown).all(axis=1)\n",
    "\t\t\tkeep_idx = keep_idx.all(axis=1)\n",
    "\t\t\tanchor_data = anchor_data[keep_idx]\n",
    "\t\t\tpositive_data = positive_data[keep_idx]\n",
    "\t\t\t# print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "\t\t\trandom_batches = random.sample(range(anchor_data.shape[0]),anchor_data.shape[0])\n",
    "\t\t\tfor batch in range(math.ceil(anchor_data.shape[0]/batch_size)):\n",
    "\n",
    "\t\t\t\t# GET BATCH DATA\n",
    "\t\t\t\trandom_batch = random_batches[batch*batch_size:(batch+1)*batch_size]\n",
    "\t\t\t\tbatch_anchor_data = torch.from_numpy(anchor_data[random_batch]).to(device)\n",
    "\t\t\t\tbatch_positive_data = torch.from_numpy(positive_data[random_batch]).to(device)\n",
    "\t\t\t\tbatch_input = torch.cat((batch_anchor_data[:,:,dynamic_channels+output_channels], batch_positive_data[:,:,dynamic_channels+output_channels]), dim=0)\n",
    "\t\t\t\tbatch_static = torch.cat((batch_anchor_data[:,0,static_channels], batch_positive_data[:,0,static_channels]), axis=0)\n",
    "\t\t\t\t# print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "\t\t\t\t# GET OUTPUT\n",
    "\t\t\t\tbatch_code_vec, batch_static_pred, batch_input_pred = model(x=batch_input)\n",
    "\t\t\t\t# print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "\t\t\t\t# CALCULATE LOSS\n",
    "\t\t\t\tbatch_recon_loss = mse_criterion(batch_input_pred, batch_input)\t\t\t\t\t\t\t\t# RECON LOSS\n",
    "\t\t\t\tbatch_recon_loss = torch.sum(batch_recon_loss, axis=2)\t\t\t\t\t\t\t\t\t\t# RECON LOSS\n",
    "\t\t\t\tbatch_recon_loss = torch.mean(batch_recon_loss)\t\t\t\t\t\t\t\t\t\t\t\t# RECON LOSS\n",
    "\t\t\t\tbatch_contrastive_loss = contrastive_criterion(batch_code_vec)\t\t\t\t\t\t\t\t# CONTRASTIVE LOSS\n",
    "\t\t\t\tbatch_static_loss = torch.mean(mse_criterion(batch_static_pred, batch_static), axis=1)\t\t# INVERSE LOSS\n",
    "\t\t\t\tbatch_static_loss = torch.mean(batch_static_loss)\t\t\t\t\t\t\t\t\t\t\t# INVERSE LOSS\n",
    "\t\t\t\tbatch_loss = (recon_weight*batch_recon_loss + contrastive_weight*batch_contrastive_loss + static_weight*batch_static_loss)/sum_weight\n",
    "\t\t\t\t# print(batch_loss.shape, batch_loss)\n",
    "\n",
    "\t\t\t\t# AGGREGATE LOSS\n",
    "\t\t\t\tepoch_loss += batch_loss.item()\n",
    "\t\t\t\tepoch_recon_loss += batch_recon_loss.item()\n",
    "\t\t\t\tepoch_contrastive_loss += batch_contrastive_loss.item()\n",
    "\t\t\t\tepoch_static_loss += batch_static_loss.item()\n",
    "\n",
    "\t\tepoch_loss /= ((batch+1)*(year+1))\n",
    "\t\tepoch_recon_loss /= ((batch+1)*(year+1))\n",
    "\t\tepoch_contrastive_loss /= ((batch+1)*(year+1))\n",
    "\t\tepoch_static_loss /= ((batch+1)*(year+1))\n",
    "\t\tprint('Valid Loss:{:.4f}\\tRecon Loss:{:.4f}\\tCont Loss:{:.4f}\\tStatic Loss:{:.4f}\\tMin Loss:{:.4f}'.format(epoch_loss, epoch_recon_loss, epoch_contrastive_loss, epoch_static_loss, min_loss))\n",
    "\t\tvalid_loss.append(epoch_loss)\n",
    "\t\tif min_loss>epoch_loss:\n",
    "\t\t\tmin_loss = epoch_loss\n",
    "\t\t\ttorch.save(model.state_dict(), os.path.join(MODEL_DIR, model_name))\n",
    "\n",
    "\t# PLOT LOSS\n",
    "\tfig = plt.figure(figsize=(10,10))\n",
    "\tax1 = fig.add_subplot(111)\n",
    "\tax1.set_xlabel(\"#Epoch\", fontsize=50)\n",
    "\n",
    "\t# PLOT TRAIN LOSS\n",
    "\tlns1 = ax1.plot(train_loss, color='red', marker='o', linewidth=4, label=\"TRAIN LOSS\")\n",
    "\n",
    "\t# PLOT VALIDATION LOSS\n",
    "\tax2 = ax1.twinx()\n",
    "\tlns2 = ax2.plot(valid_loss, color='blue', marker='o', linewidth=4, label=\"VAL LOSS\")\n",
    "\n",
    "\t# added these three lines\n",
    "\tlns = lns1+lns2\n",
    "\tlabs = [l.get_label() for l in lns]\n",
    "\tax1.legend(lns, labs, loc=\"upper right\", fontsize=40, frameon=False)\n",
    "\n",
    "\tplt.tight_layout(pad=0.0,h_pad=0.0,w_pad=0.0)\n",
    "\tplt.savefig(os.path.join(RESULT_DIR, \"{}_SCORE.pdf\".format(model_name)), format = \"pdf\")\n",
    "\tplt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
