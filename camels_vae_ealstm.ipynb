{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import config\n",
    "import MODEL\n",
    "import UTILS\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME SERIES INFO\n",
    "window = config.window\n",
    "stride = config.stride\n",
    "\n",
    "# CHANNELS INFO\n",
    "dynamic_channels = config.dynamic_channels\n",
    "static_channels = config.static_channels\n",
    "output_channels = config.output_channels\n",
    "\n",
    "# LABELS INFO\n",
    "unknown = config.unknown\n",
    "\n",
    "# MODEL INFO\n",
    "model_name = \"vae_ealstm\"\n",
    "code_dim = config.code_dim\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(config.device)\n",
    "contrastive_weight = config.contrastive_weight\n",
    "forward_weight = config.forward_weight\n",
    "sum_weight = contrastive_weight+forward_weight\n",
    "\n",
    "# TRAIN INFO\n",
    "train = config.train\n",
    "batch_size = config.batch_size\n",
    "epochs = config.epochs\n",
    "learning_rate = config.learning_rate\n",
    "train_percent = config.train_percent\n",
    "\n",
    "print(\"Hyperparameters:{}\".format(model_name))\n",
    "print(\"window : {}\".format(window))\n",
    "print(\"dynamic_channels : {}\".format(dynamic_channels))\n",
    "print(\"static_channels : {}\".format(static_channels))\n",
    "print(\"output_channels : {}\".format(output_channels))\n",
    "print(\"unknown : {}\".format(unknown))\n",
    "print(\"model_name : {}\".format(model_name))\n",
    "print(\"code_dim : {}\".format(code_dim))\n",
    "print(\"device : {}\".format(device))\n",
    "print(\"contrastive_weight : {}\".format(contrastive_weight))\n",
    "print(\"forward_weight : {}\".format(forward_weight))\n",
    "print(\"train : {}\".format(train))\n",
    "print(\"batch_size : {}\".format(batch_size))\n",
    "print(\"epochs : {}\".format(epochs))\n",
    "print(\"learning_rate : {}\".format(learning_rate))\n",
    "print(\"train_percent : {}\".format(train_percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE DIRECTORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSED_DIR = config.PREPROCESSED_DIR\n",
    "RESULT_DIR = config.RESULT_DIR\n",
    "MODEL_DIR = config.MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file):\n",
    "\tdataset = np.load(os.path.join(PREPROCESSED_DIR, \"{}.npz\".format(file)), allow_pickle=True)\n",
    "\treturn dataset\n",
    "\n",
    "def get_data(dataset, index, preprocessed=True):\n",
    "\tdata = dataset[\"data\"]\n",
    "\tif preprocessed:\n",
    "\t\tdata = (data-dataset[\"train_data_means\"])/dataset[\"train_data_stds\"]\n",
    "\tdata = np.nan_to_num(data, nan=unknown)\n",
    "\tdata = data[dataset[index]]\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_model = getattr(MODEL, \"vae\")(input_channels=len(dynamic_channels)+len(output_channels), code_dim=code_dim, output_channels=len(static_channels), device=device)\n",
    "inverse_model = inverse_model.to(device)\n",
    "pytorch_total_params = sum(p.numel() for p in inverse_model.parameters() if p.requires_grad)\n",
    "print(inverse_model)\n",
    "forward_model = getattr(MODEL, \"ealstm\")(input_dynamic_channels=len(dynamic_channels), input_static_channels=code_dim, code_dim=code_dim, output_channels=len(output_channels))\n",
    "forward_model = forward_model.to(device)\n",
    "pytorch_total_params += sum(p.numel() for p in forward_model.parameters() if p.requires_grad)\n",
    "print(forward_model)\n",
    "print(\"#Parameters:{}\".format(pytorch_total_params))\n",
    "mse_criterion = torch.nn.MSELoss(reduction=\"none\")\n",
    "kl_criterion = MODEL.KLLoss()\n",
    "optimizer = torch.optim.Adam(list(inverse_model.parameters())+list(forward_model.parameters()), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "\n",
    "\ttrain_loss = []\n",
    "\tvalid_loss = []\n",
    "\tmin_loss = 10000\n",
    "\n",
    "\tfor epoch in range(1,epochs+1):\n",
    "\n",
    "\t\t# LOAD DATA\n",
    "\t\tfile, index = \"strided_train\", \"train_index\"\n",
    "\t\tdataset = load_dataset(file)\n",
    "\t\tdata = get_data(dataset, index)\n",
    "\t\tnodes, years, window, channels = data.shape\n",
    "\t\t# print(nodes, years, window, channels)\n",
    "\n",
    "\t\t# GET ANCHOR AND POSITIVE YEARS\n",
    "\t\tanchor_years = np.zeros((nodes, years))\n",
    "\t\tfor node in range(nodes):\n",
    "\t\t\tanchor_years[node] = random.sample(range(years), years)\n",
    "\t\tanchor_years = anchor_years.astype(np.int64)\n",
    "\t\tpositive_years = np.zeros((nodes, years))\n",
    "\t\tfor node in range(nodes):\n",
    "\t\t\tpositive_years[node] = random.sample(range(years), years)\n",
    "\t\tpositive_years = positive_years.astype(np.int64)\n",
    "\t\t# print(anchor_years.shape, positive_years.shape)\n",
    "\n",
    "\t\t# GET TRAIN AND VALIDATION SET\n",
    "\t\tanchor_years_train = anchor_years[:,:int(train_percent*years)]\n",
    "\t\tanchor_years_valid = anchor_years[:,int(train_percent*years):]\n",
    "\t\tpositive_years_train = positive_years[:,:int(train_percent*years)]\n",
    "\t\tpositive_years_valid = positive_years[:,int(train_percent*years):]\n",
    "\t\t# print(anchor_years_train.shape, anchor_years_valid.shape, positive_years_train.shape, positive_years_valid.shape)\n",
    "\n",
    "\t\t# LOSS ON TRAIN SET\n",
    "\t\tinverse_model.train()\n",
    "\t\tforward_model.train()\n",
    "\t\tepoch_loss = 0\n",
    "\t\tepoch_kl_loss = 0\n",
    "\t\tepoch_forward_loss = 0\n",
    "\t\tfor year in range(anchor_years_train.shape[1]):\n",
    "\n",
    "\t\t\t#Get (anchor,positive) Instances for each node\n",
    "\t\t\tanchor_data = data[np.arange(nodes), anchor_years_train[:, year]]\n",
    "\t\t\tpositive_data = data[np.arange(nodes), positive_years_train[:, year]]\n",
    "\t\t\t# print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "\t\t\t# Remove pairs where (anchor,positive) years are same\n",
    "\t\t\tkeep_idx = anchor_years_train[:, year] != positive_years_train[:, year]\n",
    "\t\t\tanchor_data = anchor_data[keep_idx]\n",
    "\t\t\tpositive_data = positive_data[keep_idx]\n",
    "\t\t\t# print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "\t\t\t# Remove pairs where (anchor,positive) basins have unknown in streamflow\n",
    "\t\t\tkeep_idx = np.zeros((anchor_data.shape[0], 2)).astype(bool)\n",
    "\t\t\tkeep_idx[:,0] = (anchor_data[:,:,-1]!=unknown).all(axis=1)\n",
    "\t\t\tkeep_idx[:,1] = (positive_data[:,:,-1]!=unknown).all(axis=1)\n",
    "\t\t\tkeep_idx = keep_idx.all(axis=1)\n",
    "\t\t\tanchor_data = anchor_data[keep_idx]\n",
    "\t\t\tpositive_data = positive_data[keep_idx]\n",
    "\t\t\t# print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "\t\t\trandom_batches = random.sample(range(anchor_data.shape[0]),anchor_data.shape[0])\n",
    "\t\t\tfor batch in range(math.ceil(anchor_data.shape[0]/batch_size)):\n",
    "\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t\t# GET BATCH DATA FOR INVERSE MODEL\n",
    "\t\t\t\trandom_batch = random_batches[batch*batch_size:(batch+1)*batch_size]\n",
    "\t\t\t\tbatch_anchor_data = torch.from_numpy(anchor_data[random_batch]).to(device)\n",
    "\t\t\t\tbatch_input = batch_anchor_data[:,:,dynamic_channels+output_channels].to(device)\n",
    "\t\t\t\tbatch_static = batch_anchor_data[:,0,static_channels].to(device)\n",
    "\t\t\t\t# print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "\t\t\t\t# GET INVERSE OUTPUT\n",
    "\t\t\t\tbatch_code_vec, batch_mu, batch_std, batch_static_pred, batch_input_pred = inverse_model(x=batch_input)\n",
    "\t\t\t\t# print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "\t\t\t\t# GET BATCH DATA FOR FORWARD MODEL\n",
    "\t\t\t\tbatch_positive_data = torch.from_numpy(positive_data[random_batch]).to(device)\n",
    "\t\t\t\tbatch_dynamic_input = batch_positive_data[:, :, dynamic_channels].to(device)\n",
    "\t\t\t\tbatch_static_input = batch_code_vec\n",
    "\t\t\t\tbatch_label = batch_positive_data[:, :, output_channels].to(device)\n",
    "\t\t\t\t# print(batch_dynamic_input.shape, batch_static_input.shape, batch_label.shape)\n",
    "\n",
    "\t\t\t\t# GET FORWARD OUTPUT\n",
    "\t\t\t\tbatch_label_pred = forward_model(x_dynamic=batch_dynamic_input.to(device), x_static=batch_static_input.to(device))\n",
    "\t\t\t\t# print(batch_label_pred.shape)\n",
    "\n",
    "\t\t\t\t# CALCULATE LOSS\n",
    "\t\t\t\tbatch_kl_loss = kl_criterion(batch_code_vec, batch_mu, batch_std)\t\t\t\t\t\t\t\t\t\t# KL LOSS\n",
    "\t\t\t\tbatch_forward_loss = mse_criterion(batch_label, batch_label_pred)\t\t\t\t\t\t\t\t\t\t# FORWARD LOSS (PER CHANNEL LOSS)\n",
    "\t\t\t\tmask = (batch_label!=unknown).float()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# FORWARD LOSS (CREATE MASK)\n",
    "\t\t\t\tbatch_forward_loss = batch_forward_loss * mask\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# FORWARD LOSS (MULTIPLY MASK)\n",
    "\t\t\t\tbatch_forward_loss, mask = torch.sum(batch_forward_loss, dim=2), (torch.sum(mask, dim=2)>0).float()\t\t# FORWARD LOSS (PER INSTANCE LOSS)\n",
    "\t\t\t\tbatch_forward_loss = torch.sum(batch_forward_loss)/torch.sum(mask)\t\t\t\t\t\t\t\t\t\t# FORWARD LOSS (MEAN SEQUENCE LOSS)\n",
    "\t\t\t\tbatch_loss = (contrastive_weight*batch_kl_loss + forward_weight*batch_forward_loss)/sum_weight\n",
    "\t\t\t\t# print(batch_loss.shape, batch_loss)\n",
    "\n",
    "\t\t\t\t# LOSS BACKPROPOGATE\n",
    "\t\t\t\tbatch_loss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t\t# AGGREGATE LOSS\n",
    "\t\t\t\tepoch_loss += batch_loss.item()\n",
    "\t\t\t\tepoch_kl_loss += batch_kl_loss.item()\n",
    "\t\t\t\tepoch_forward_loss += batch_forward_loss.item()\n",
    "\n",
    "\t\tepoch_loss /= ((batch+1)*(year+1))\n",
    "\t\tepoch_kl_loss /= ((batch+1)*(year+1))\n",
    "\t\tepoch_forward_loss /= ((batch+1)*(year+1))\n",
    "\t\tprint('Epoch:{}\\tTrain Loss:{:.4f}\\tKL Loss:{:.4f}\\tForward Loss:{:.4f}'.format(epoch, epoch_loss, epoch_kl_loss, epoch_forward_loss), end=\"\\t\")\n",
    "\t\ttrain_loss.append(epoch_loss)\n",
    "\n",
    "\t\t# LOSS ON VALIDATION SET\n",
    "\t\tinverse_model.eval()\n",
    "\t\tforward_model.eval()\n",
    "\t\tepoch_loss = 0\n",
    "\t\tepoch_kl_loss = 0\n",
    "\t\tepoch_forward_loss = 0\n",
    "\t\tfor year in range(anchor_years_valid.shape[1]):\n",
    "\n",
    "\t\t\t#Get (anchor,positive) Instances for each node\n",
    "\t\t\tanchor_data = data[np.arange(nodes), anchor_years_valid[:, year]]\n",
    "\t\t\tpositive_data = data[np.arange(nodes), positive_years_valid[:, year]]\n",
    "\t\t\t# print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "\t\t\t# Remove pairs where (anchor,positive) years are same\n",
    "\t\t\tkeep_idx = anchor_years_valid[:, year] != positive_years_valid[:, year]\n",
    "\t\t\tanchor_data = anchor_data[keep_idx]\n",
    "\t\t\tpositive_data = positive_data[keep_idx]\n",
    "\t\t\t# print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "\t\t\t# Remove pairs where (anchor,positive) basins have unknown in streamflow\n",
    "\t\t\tkeep_idx = np.zeros((anchor_data.shape[0], 2)).astype(bool)\n",
    "\t\t\tkeep_idx[:,0] = (anchor_data[:,:,-1]!=unknown).all(axis=1)\n",
    "\t\t\tkeep_idx[:,1] = (positive_data[:,:,-1]!=unknown).all(axis=1)\n",
    "\t\t\tkeep_idx = keep_idx.all(axis=1)\n",
    "\t\t\tanchor_data = anchor_data[keep_idx]\n",
    "\t\t\tpositive_data = positive_data[keep_idx]\n",
    "\t\t\t# print(anchor_data.shape, positive_data.shape)\n",
    "\n",
    "\t\t\trandom_batches = random.sample(range(anchor_data.shape[0]),anchor_data.shape[0])\n",
    "\t\t\tfor batch in range(math.ceil(anchor_data.shape[0]/batch_size)):\n",
    "\n",
    "\t\t\t\t# GET BATCH DATA FOR INVERSE MODEL\n",
    "\t\t\t\trandom_batch = random_batches[batch*batch_size:(batch+1)*batch_size]\n",
    "\t\t\t\tbatch_anchor_data = torch.from_numpy(anchor_data[random_batch]).to(device)\n",
    "\t\t\t\tbatch_input = batch_anchor_data[:,:,dynamic_channels+output_channels].to(device)\n",
    "\t\t\t\tbatch_static = batch_anchor_data[:,0,static_channels].to(device)\n",
    "\t\t\t\t# print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "\t\t\t\t# GET INVERSE OUTPUT\n",
    "\t\t\t\tbatch_code_vec, batch_mu, batch_std, batch_static_pred, batch_input_pred = inverse_model(x=batch_input)\n",
    "\t\t\t\t# print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "\t\t\t\t# GET BATCH DATA FOR FORWARD MODEL\n",
    "\t\t\t\tbatch_positive_data = torch.from_numpy(positive_data[random_batch]).to(device)\n",
    "\t\t\t\tbatch_dynamic_input = batch_positive_data[:, :, dynamic_channels].to(device)\n",
    "\t\t\t\tbatch_static_input = batch_code_vec\n",
    "\t\t\t\tbatch_label = batch_positive_data[:, :, output_channels].to(device)\n",
    "\t\t\t\t# print(batch_dynamic_input.shape, batch_static_input.shape, batch_label.shape)\n",
    "\n",
    "\t\t\t\t# GET FORWARD OUTPUT\n",
    "\t\t\t\tbatch_label_pred = forward_model(x_dynamic=batch_dynamic_input.to(device), x_static=batch_static_input.to(device))\n",
    "\t\t\t\t# print(batch_label_pred.shape)\n",
    "\n",
    "\t\t\t\t# CALCULATE LOSS\n",
    "\t\t\t\tbatch_kl_loss = kl_criterion(batch_code_vec, batch_mu, batch_std)\t\t\t\t\t\t\t\t\t\t# KL LOSS\n",
    "\t\t\t\tbatch_forward_loss = mse_criterion(batch_label, batch_label_pred)\t\t\t\t\t\t\t\t\t\t# FORWARD LOSS (PER CHANNEL LOSS)\n",
    "\t\t\t\tmask = (batch_label!=unknown).float()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# FORWARD LOSS (CREATE MASK)\n",
    "\t\t\t\tbatch_forward_loss = batch_forward_loss * mask\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# FORWARD LOSS (MULTIPLY MASK)\n",
    "\t\t\t\tbatch_forward_loss, mask = torch.sum(batch_forward_loss, dim=2), (torch.sum(mask, dim=2)>0).float()\t\t# FORWARD LOSS (PER INSTANCE LOSS)\n",
    "\t\t\t\tbatch_forward_loss = torch.sum(batch_forward_loss)/torch.sum(mask)\t\t\t\t\t\t\t\t\t\t# FORWARD LOSS (MEAN SEQUENCE LOSS)\n",
    "\t\t\t\tbatch_loss = (contrastive_weight*batch_kl_loss + forward_weight*batch_forward_loss)/sum_weight\n",
    "\t\t\t\t# print(batch_loss.shape, batch_loss)\n",
    "\n",
    "\t\t\t\t# AGGREGATE LOSS\n",
    "\t\t\t\tepoch_loss += batch_loss.item()\n",
    "\t\t\t\tepoch_kl_loss += batch_kl_loss.item()\n",
    "\t\t\t\tepoch_forward_loss += batch_forward_loss.item()\n",
    "\n",
    "\t\tepoch_loss /= ((batch+1)*(year+1))\n",
    "\t\tepoch_kl_loss /= ((batch+1)*(year+1))\n",
    "\t\tepoch_forward_loss /= ((batch+1)*(year+1))\n",
    "\t\tprint('Valid Loss:{:.4f}\\tKL Loss:{:.4f}\\tForward Loss:{:.4f}\\tMin Loss:{:.4f}'.format(epoch_loss, epoch_kl_loss, epoch_forward_loss, min_loss))\n",
    "\t\tvalid_loss.append(epoch_loss)\n",
    "\t\tif min_loss>epoch_loss:\n",
    "\t\t\tmin_loss = epoch_loss\n",
    "\t\t\ttorch.save(inverse_model.state_dict(), os.path.join(MODEL_DIR, \"{}_inverse\".format(model_name)))\n",
    "\t\t\ttorch.save(forward_model.state_dict(), os.path.join(MODEL_DIR, \"{}_forward\".format(model_name)))\n",
    "\n",
    "\t# PLOT LOSS\n",
    "\tfig = plt.figure(figsize=(10,10))\n",
    "\tax1 = fig.add_subplot(111)\n",
    "\tax1.set_xlabel(\"#Epoch\", fontsize=50)\n",
    "\n",
    "\t# PLOT TRAIN LOSS\n",
    "\tlns1 = ax1.plot(train_loss, color='red', marker='o', linewidth=4, label=\"TRAIN LOSS\")\n",
    "\n",
    "\t# PLOT VALIDATION SCORE\n",
    "\tax2 = ax1.twinx()\n",
    "\tlns2 = ax2.plot(valid_loss, color='blue', marker='o', linewidth=4, label=\"VAL LOSS\")\n",
    "\n",
    "\t# added these three lines\n",
    "\tlns = lns1+lns2\n",
    "\tlabs = [l.get_label() for l in lns]\n",
    "\tax1.legend(lns, labs, loc=\"upper right\", fontsize=40, frameon=False)\n",
    "\n",
    "\tplt.tight_layout(pad=0.0,h_pad=0.0,w_pad=0.0)\n",
    "\tplt.savefig(os.path.join(RESULT_DIR, \"{}_SCORE.pdf\".format(model_name)), format = \"pdf\")\n",
    "\tplt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_model.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"{}_inverse\".format(model_name))))\n",
    "inverse_model.eval()\n",
    "forward_model.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"{}_forward\".format(model_name))))\n",
    "forward_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST MODEL (AVERAGED EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IN DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file, index = \"strided_train\", \"train_index\"\n",
    "dataset = load_dataset(file)\n",
    "data = get_data(dataset, index)\n",
    "nodes, years, window, channels = data.shape\n",
    "# print(nodes, years, window, channels)\n",
    "\n",
    "dataset_code = unknown*np.ones((nodes, years, code_dim), dtype=np.float32)\n",
    "for year in range(years):\n",
    "\n",
    "\t#Get instances for each node\n",
    "\tnode_data = data[np.arange(nodes), year]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\t# Remove pairs where node have unknown in streamflow\n",
    "\tkeep_idx = np.zeros((node_data.shape[0], 1)).astype(bool)\n",
    "\tkeep_idx[:,0] = (node_data[:,:,-1]!=unknown).all(axis=1)\n",
    "\tkeep_idx = keep_idx.all(axis=1)\n",
    "\tnode_data = node_data[keep_idx]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\tfor batch in range(math.ceil(node_data.shape[0]/batch_size)):\n",
    "\n",
    "\t\t# GET BATCH DATA FOR INVERSE MODEL\n",
    "\t\tbatch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_input = batch_data[:,:,dynamic_channels+output_channels]\n",
    "\t\tbatch_static = batch_data[:,0,static_channels]\n",
    "\t\t# print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "\t\t# GET INVERSE OUTPUT\n",
    "\t\tbatch_code_vec, batch_mu, batch_std, batch_static_pred, batch_input_pred = inverse_model(x=batch_input)\n",
    "\t\t# print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "\t\t# STORE OUTPUT\n",
    "\t\tdataset_code[batch*batch_size:(batch+1)*batch_size, year] = batch_code_vec.detach().cpu().numpy()\n",
    "dataset_code = np.mean(dataset_code, axis=1)\n",
    "\n",
    "file, index = \"strided_test\", \"train_index\"\n",
    "dataset = load_dataset(file)\n",
    "data = get_data(dataset, index)\n",
    "nodes, years, window, channels = data.shape\n",
    "# print(nodes, years, window, channels)\n",
    "\n",
    "dataset_true = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "dataset_pred = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "for year in range(years):\n",
    "\n",
    "\t#Get instance for each node\n",
    "\tnode_data = data[np.arange(nodes), year]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\tfor batch in range(math.ceil(nodes/batch_size)):\n",
    "\n",
    "\t\t# GET BATCH DATA FOR FORWARD MODEL\n",
    "\t\tbatch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_dynamic_input = batch_data[:, :, dynamic_channels].to(device)\n",
    "\t\tbatch_static_input = torch.from_numpy(dataset_code[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_label = batch_data[:, :, output_channels].to(device)\n",
    "\t\t# print(batch_dynamic_input.shape, batch_static_input.shape, batch_label.shape)\n",
    "\n",
    "\t\t# GET FORWARD OUTPUT\n",
    "\t\tbatch_label_pred = forward_model(x_dynamic=batch_dynamic_input.to(device), x_static=batch_static_input.to(device))\n",
    "\t\t# print(batch_label_pred.shape)\n",
    "\n",
    "\t\t# STORE OUTPUT\n",
    "\t\tdataset_true[batch*batch_size:(batch+1)*batch_size, year] = batch_label.detach().cpu().numpy()\n",
    "\t\tdataset_pred[batch*batch_size:(batch+1)*batch_size, year] = batch_label_pred.detach().cpu().numpy()\n",
    "\n",
    "dataset_true = (dataset_true*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "dataset_pred = (dataset_pred*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "dataset_true = UTILS.unstride_array(dataset_true)\n",
    "dataset_pred = UTILS.unstride_array(dataset_pred)\n",
    "dataset_true = dataset_true[:, stride:]\n",
    "dataset_pred = dataset_pred[:, stride:]\n",
    "\n",
    "per_sample_RMSE = UTILS.per_sample_RMSE(dataset_true, dataset_pred, unknown)\n",
    "_, per_node_RMSE = UTILS.per_node_RMSE(dataset_true, dataset_pred, unknown)\n",
    "per_sample_R2 = UTILS.per_sample_R2(dataset_true, dataset_pred, unknown)\n",
    "_, per_node_R2 = UTILS.per_node_R2(dataset_true, dataset_pred, unknown)\n",
    "print(\"Per Sample RMSE:{:.4f}\\tPer Node RMSE:{:.4f}\\tPer Sample R2:{:.4f}\\tPer Node R2:{:.4f}\".format(per_sample_RMSE, per_node_RMSE, per_sample_R2, per_node_R2))\n",
    "np.save(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"true\")), dataset_true)\n",
    "np.save(os.path.join(RESULT_DIR, \"{}_{}_{}_{}\".format(file, index, model_name, \"averaged\")), dataset_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUT DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file, index = \"strided_train\", \"test_index\"\n",
    "dataset = load_dataset(file)\n",
    "data = get_data(dataset, index)\n",
    "nodes, years, window, channels = data.shape\n",
    "# print(nodes, years, window, channels)\n",
    "\n",
    "dataset_code = unknown*np.ones((nodes, years, code_dim), dtype=np.float32)\n",
    "for year in range(years):\n",
    "\n",
    "\t#Get instances for each node\n",
    "\tnode_data = data[np.arange(nodes), year]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\t# Remove pairs where node have unknown in streamflow\n",
    "\tkeep_idx = np.zeros((node_data.shape[0], 1)).astype(bool)\n",
    "\tkeep_idx[:,0] = (node_data[:,:,-1]!=unknown).all(axis=1)\n",
    "\tkeep_idx = keep_idx.all(axis=1)\n",
    "\tnode_data = node_data[keep_idx]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\tfor batch in range(math.ceil(node_data.shape[0]/batch_size)):\n",
    "\n",
    "\t\t# GET BATCH DATA FOR INVERSE MODEL\n",
    "\t\tbatch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_input = batch_data[:,:,dynamic_channels+output_channels]\n",
    "\t\tbatch_static = batch_data[:,0,static_channels]\n",
    "\t\t# print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "\t\t# GET INVERSE OUTPUT\n",
    "\t\tbatch_code_vec, batch_mu, batch_std, batch_static_pred, batch_input_pred = inverse_model(x=batch_input)\n",
    "\t\t# print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "\t\t# STORE OUTPUT\n",
    "\t\tdataset_code[batch*batch_size:(batch+1)*batch_size, year] = batch_code_vec.detach().cpu().numpy()\n",
    "dataset_code = np.mean(dataset_code, axis=1)\n",
    "\n",
    "file, index = \"strided_test\", \"test_index\"\n",
    "dataset = load_dataset(file)\n",
    "data = get_data(dataset, index)\n",
    "nodes, years, window, channels = data.shape\n",
    "# print(nodes, years, window, channels)\n",
    "\n",
    "dataset_true = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "dataset_pred = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "for year in range(years):\n",
    "\n",
    "\t#Get instance for each node\n",
    "\tnode_data = data[np.arange(nodes), year]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\tfor batch in range(math.ceil(nodes/batch_size)):\n",
    "\n",
    "\t\t# GET BATCH DATA FOR FORWARD MODEL\n",
    "\t\tbatch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_dynamic_input = batch_data[:, :, dynamic_channels].to(device)\n",
    "\t\tbatch_static_input = torch.from_numpy(dataset_code[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_label = batch_data[:, :, output_channels].to(device)\n",
    "\t\t# print(batch_dynamic_input.shape, batch_static_input.shape, batch_label.shape)\n",
    "\n",
    "\t\t# GET FORWARD OUTPUT\n",
    "\t\tbatch_label_pred = forward_model(x_dynamic=batch_dynamic_input.to(device), x_static=batch_static_input.to(device))\n",
    "\t\t# print(batch_label_pred.shape)\n",
    "\n",
    "\t\t# STORE OUTPUT\n",
    "\t\tdataset_true[batch*batch_size:(batch+1)*batch_size, year] = batch_label.detach().cpu().numpy()\n",
    "\t\tdataset_pred[batch*batch_size:(batch+1)*batch_size, year] = batch_label_pred.detach().cpu().numpy()\n",
    "\n",
    "dataset_true = (dataset_true*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "dataset_pred = (dataset_pred*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "dataset_true = UTILS.unstride_array(dataset_true)\n",
    "dataset_pred = UTILS.unstride_array(dataset_pred)\n",
    "dataset_true = dataset_true[:, stride:]\n",
    "dataset_pred = dataset_pred[:, stride:]\n",
    "\n",
    "per_sample_RMSE = UTILS.per_sample_RMSE(dataset_true, dataset_pred, unknown)\n",
    "_, per_node_RMSE = UTILS.per_node_RMSE(dataset_true, dataset_pred, unknown)\n",
    "per_sample_R2 = UTILS.per_sample_R2(dataset_true, dataset_pred, unknown)\n",
    "_, per_node_R2 = UTILS.per_node_R2(dataset_true, dataset_pred, unknown)\n",
    "print(\"Per Sample RMSE:{:.4f}\\tPer Node RMSE:{:.4f}\\tPer Sample R2:{:.4f}\\tPer Node R2:{:.4f}\".format(per_sample_RMSE, per_node_RMSE, per_sample_R2, per_node_R2))\n",
    "np.save(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"true\")), dataset_true)\n",
    "np.save(os.path.join(RESULT_DIR, \"{}_{}_{}_{}\".format(file, index, model_name, \"averaged\")), dataset_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST MODEL (YEARLY AVERAGED EMBEDDINGS I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IN DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file, index = \"strided_train\", \"train_index\"\n",
    "dataset = load_dataset(file)\n",
    "data = get_data(dataset, index)\n",
    "nodes, years, window, channels = data.shape\n",
    "# print(nodes, years, window, channels)\n",
    "\n",
    "dataset_code = np.nan*np.ones((nodes, years, code_dim), dtype=np.float32)\n",
    "for year in range(years):\n",
    "\n",
    "\t#Get instances for each node\n",
    "\tnode_data = data[np.arange(nodes), year]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\t# Remove pairs where node have unknown in streamflow\n",
    "\tkeep_idx = np.zeros((node_data.shape[0], 1)).astype(bool)\n",
    "\tkeep_idx[:,0] = (node_data[:,:,-1]!=unknown).all(axis=1)\n",
    "\tkeep_idx = keep_idx.all(axis=1)\n",
    "\tnode_data = node_data[keep_idx]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\tfor batch in range(math.ceil(node_data.shape[0]/batch_size)):\n",
    "\n",
    "\t\t# GET BATCH DATA FOR INVERSE MODEL\n",
    "\t\tbatch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_input = batch_data[:,:,dynamic_channels+output_channels]\n",
    "\t\tbatch_static = batch_data[:,0,static_channels]\n",
    "\t\t# print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "\t\t# GET INVERSE OUTPUT\n",
    "\t\tbatch_code_vec, batch_mu, batch_std, batch_static_pred, batch_input_pred = inverse_model(x=batch_input)\n",
    "\t\t# print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "\t\t# STORE OUTPUT\n",
    "\t\tdataset_code[batch*batch_size:(batch+1)*batch_size, year] = batch_code_vec.detach().cpu().numpy()\n",
    "\n",
    "file, index = \"strided_test\", \"train_index\"\n",
    "dataset = load_dataset(file)\n",
    "data = get_data(dataset, index)\n",
    "nodes, years, window, channels = data.shape\n",
    "# print(nodes, years, window, channels)\n",
    "\n",
    "dataset_true = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "dataset_pred = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "for year in range(years):\n",
    "\n",
    "\t# COLLECT EMBEDDINGS\n",
    "\tyear_code = np.nan*np.ones((nodes, code_dim), dtype=np.float32)\n",
    "\n",
    "\t#Get instance for each node\n",
    "\tnode_data = data[np.arange(nodes), year]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\tfor batch in range(math.ceil(nodes/batch_size)):\n",
    "\n",
    "\t\t# GET BATCH DATA FOR FORWARD MODEL\n",
    "\t\tbatch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_dynamic_input = batch_data[:, :, dynamic_channels].to(device)\n",
    "\t\tbatch_static_input = torch.from_numpy(np.nanmean(dataset_code, axis=1)[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_label = batch_data[:, :, output_channels].to(device)\n",
    "\t\t# print(batch_dynamic_input.shape, batch_static_input.shape, batch_label.shape)\n",
    "\n",
    "\t\t# GET FORWARD OUTPUT\n",
    "\t\tbatch_label_pred = forward_model(x_dynamic=batch_dynamic_input.to(device), x_static=batch_static_input.to(device))\n",
    "\t\t# print(batch_label_pred.shape)\n",
    "\n",
    "\t\t# STORE FORWARD OUTPUT\n",
    "\t\tdataset_true[batch*batch_size:(batch+1)*batch_size, year] = batch_label.detach().cpu().numpy()\n",
    "\t\tdataset_pred[batch*batch_size:(batch+1)*batch_size, year] = batch_label_pred.detach().cpu().numpy()\n",
    "\n",
    "\t\t# GET BATCH DATA FOR INVERSE MODEL\n",
    "\t\tbatch_input = batch_data[:,:,dynamic_channels+output_channels]\n",
    "\t\tbatch_static = batch_data[:,0,static_channels]\n",
    "\t\t# print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "\t\t# GET INVERSE OUTPUT\n",
    "\t\tbatch_code_vec, batch_mu, batch_std, batch_static_pred, batch_input_pred = inverse_model(x=batch_input)\n",
    "\t\t# print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "\t\t# STORE INVERSE OUTPUT\n",
    "\t\tyear_code[batch*batch_size:(batch+1)*batch_size] = batch_code_vec.detach().cpu().numpy()\n",
    "\n",
    "\tdataset_code = np.concatenate((dataset_code, np.expand_dims(year_code, axis=1)), axis=1)\n",
    "\n",
    "dataset_true = (dataset_true*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "dataset_pred = (dataset_pred*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "dataset_true = UTILS.unstride_array(dataset_true)\n",
    "dataset_pred = UTILS.unstride_array(dataset_pred)\n",
    "dataset_true = dataset_true[:, stride:]\n",
    "dataset_pred = dataset_pred[:, stride:]\n",
    "\n",
    "per_sample_RMSE = UTILS.per_sample_RMSE(dataset_true, dataset_pred, unknown)\n",
    "_, per_node_RMSE = UTILS.per_node_RMSE(dataset_true, dataset_pred, unknown)\n",
    "per_sample_R2 = UTILS.per_sample_R2(dataset_true, dataset_pred, unknown)\n",
    "_, per_node_R2 = UTILS.per_node_R2(dataset_true, dataset_pred, unknown)\n",
    "print(\"Per Sample RMSE:{:.4f}\\tPer Node RMSE:{:.4f}\\tPer Sample R2:{:.4f}\\tPer Node R2:{:.4f}\".format(per_sample_RMSE, per_node_RMSE, per_sample_R2, per_node_R2))\n",
    "np.save(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"true\")), dataset_true)\n",
    "np.save(os.path.join(RESULT_DIR, \"{}_{}_{}_{}\".format(file, index, model_name, \"yearly_averaged_I\")), dataset_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUT DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file, index = \"strided_train\", \"test_index\"\n",
    "dataset = load_dataset(file)\n",
    "data = get_data(dataset, index)\n",
    "nodes, years, window, channels = data.shape\n",
    "# print(nodes, years, window, channels)\n",
    "\n",
    "dataset_code = np.nan*np.ones((nodes, years, code_dim), dtype=np.float32)\n",
    "for year in range(years):\n",
    "\n",
    "\t#Get instances for each node\n",
    "\tnode_data = data[np.arange(nodes), year]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\t# Remove pairs where node have unknown in streamflow\n",
    "\tkeep_idx = np.zeros((node_data.shape[0], 1)).astype(bool)\n",
    "\tkeep_idx[:,0] = (node_data[:,:,-1]!=unknown).all(axis=1)\n",
    "\tkeep_idx = keep_idx.all(axis=1)\n",
    "\tnode_data = node_data[keep_idx]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\tfor batch in range(math.ceil(node_data.shape[0]/batch_size)):\n",
    "\n",
    "\t\t# GET BATCH DATA FOR INVERSE MODEL\n",
    "\t\tbatch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_input = batch_data[:,:,dynamic_channels+output_channels]\n",
    "\t\tbatch_static = batch_data[:,0,static_channels]\n",
    "\t\t# print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "\t\t# GET INVERSE OUTPUT\n",
    "\t\tbatch_code_vec, batch_mu, batch_std, batch_static_pred, batch_input_pred = inverse_model(x=batch_input)\n",
    "\t\t# print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "\t\t# STORE OUTPUT\n",
    "\t\tdataset_code[batch*batch_size:(batch+1)*batch_size, year] = batch_code_vec.detach().cpu().numpy()\n",
    "\n",
    "file, index = \"strided_test\", \"test_index\"\n",
    "dataset = load_dataset(file)\n",
    "data = get_data(dataset, index)\n",
    "nodes, years, window, channels = data.shape\n",
    "# print(nodes, years, window, channels)\n",
    "\n",
    "dataset_true = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "dataset_pred = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "for year in range(years):\n",
    "\n",
    "\t# COLLECT EMBEDDINGS\n",
    "\tyear_code = np.nan*np.ones((nodes, code_dim), dtype=np.float32)\n",
    "\n",
    "\t#Get instance for each node\n",
    "\tnode_data = data[np.arange(nodes), year]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\tfor batch in range(math.ceil(nodes/batch_size)):\n",
    "\n",
    "\t\t# GET BATCH DATA FOR FORWARD MODEL\n",
    "\t\tbatch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_dynamic_input = batch_data[:, :, dynamic_channels].to(device)\n",
    "\t\tbatch_static_input = torch.from_numpy(np.nanmean(dataset_code, axis=1)[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_label = batch_data[:, :, output_channels].to(device)\n",
    "\t\t# print(batch_dynamic_input.shape, batch_static_input.shape, batch_label.shape)\n",
    "\n",
    "\t\t# GET FORWARD OUTPUT\n",
    "\t\tbatch_label_pred = forward_model(x_dynamic=batch_dynamic_input.to(device), x_static=batch_static_input.to(device))\n",
    "\t\t# print(batch_label_pred.shape)\n",
    "\n",
    "\t\t# STORE FORWARD OUTPUT\n",
    "\t\tdataset_true[batch*batch_size:(batch+1)*batch_size, year] = batch_label.detach().cpu().numpy()\n",
    "\t\tdataset_pred[batch*batch_size:(batch+1)*batch_size, year] = batch_label_pred.detach().cpu().numpy()\n",
    "\n",
    "\t\t# GET BATCH DATA FOR INVERSE MODEL\n",
    "\t\tbatch_input = batch_data[:,:,dynamic_channels+output_channels]\n",
    "\t\tbatch_static = batch_data[:,0,static_channels]\n",
    "\t\t# print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "\t\t# GET INVERSE OUTPUT\n",
    "\t\tbatch_code_vec, batch_mu, batch_std, batch_static_pred, batch_input_pred = inverse_model(x=batch_input)\n",
    "\t\t# print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "\t\t# STORE INVERSE OUTPUT\n",
    "\t\tyear_code[batch*batch_size:(batch+1)*batch_size] = batch_code_vec.detach().cpu().numpy()\n",
    "\n",
    "\tdataset_code = np.concatenate((dataset_code, np.expand_dims(year_code, axis=1)), axis=1)\n",
    "\n",
    "dataset_true = (dataset_true*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "dataset_pred = (dataset_pred*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "dataset_true = UTILS.unstride_array(dataset_true)\n",
    "dataset_pred = UTILS.unstride_array(dataset_pred)\n",
    "dataset_true = dataset_true[:, stride:]\n",
    "dataset_pred = dataset_pred[:, stride:]\n",
    "\n",
    "per_sample_RMSE = UTILS.per_sample_RMSE(dataset_true, dataset_pred, unknown)\n",
    "_, per_node_RMSE = UTILS.per_node_RMSE(dataset_true, dataset_pred, unknown)\n",
    "per_sample_R2 = UTILS.per_sample_R2(dataset_true, dataset_pred, unknown)\n",
    "_, per_node_R2 = UTILS.per_node_R2(dataset_true, dataset_pred, unknown)\n",
    "print(\"Per Sample RMSE:{:.4f}\\tPer Node RMSE:{:.4f}\\tPer Sample R2:{:.4f}\\tPer Node R2:{:.4f}\".format(per_sample_RMSE, per_node_RMSE, per_sample_R2, per_node_R2))\n",
    "np.save(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"true\")), dataset_true)\n",
    "np.save(os.path.join(RESULT_DIR, \"{}_{}_{}_{}\".format(file, index, model_name, \"yearly_averaged_I\")), dataset_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST MODEL (YEARLY AVERAGED EMBEDDINGS II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IN DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file, index = \"strided_train\", \"train_index\"\n",
    "dataset = load_dataset(file)\n",
    "data = get_data(dataset, index)\n",
    "nodes, years, window, channels = data.shape\n",
    "# print(nodes, years, window, channels)\n",
    "\n",
    "dataset_code = np.nan*np.ones((nodes, years, code_dim), dtype=np.float32)\n",
    "for year in range(years):\n",
    "\n",
    "\t#Get instances for each node\n",
    "\tnode_data = data[np.arange(nodes), year]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\t# Remove pairs where node have unknown in streamflow\n",
    "\tkeep_idx = np.zeros((node_data.shape[0], 1)).astype(bool)\n",
    "\tkeep_idx[:,0] = (node_data[:,:,-1]!=unknown).all(axis=1)\n",
    "\tkeep_idx = keep_idx.all(axis=1)\n",
    "\tnode_data = node_data[keep_idx]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\tfor batch in range(math.ceil(node_data.shape[0]/batch_size)):\n",
    "\n",
    "\t\t# GET BATCH DATA FOR INVERSE MODEL\n",
    "\t\tbatch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_input = batch_data[:,:,dynamic_channels+output_channels]\n",
    "\t\tbatch_static = batch_data[:,0,static_channels]\n",
    "\t\t# print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "\t\t# GET INVERSE OUTPUT\n",
    "\t\tbatch_code_vec, batch_mu, batch_std, batch_static_pred, batch_input_pred = inverse_model(x=batch_input)\n",
    "\t\t# print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "\t\t# STORE OUTPUT\n",
    "\t\tdataset_code[batch*batch_size:(batch+1)*batch_size, year] = batch_code_vec.detach().cpu().numpy()\n",
    "dataset_code = np.nanmean(dataset_code, axis=1)\n",
    "\n",
    "file, index = \"strided_test\", \"train_index\"\n",
    "dataset = load_dataset(file)\n",
    "data = get_data(dataset, index)\n",
    "nodes, years, window, channels = data.shape\n",
    "# print(nodes, years, window, channels)\n",
    "\n",
    "dataset_true = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "dataset_pred = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "for year in range(years):\n",
    "\n",
    "\t# COLLECT EMBEDDINGS\n",
    "\tyear_code = np.nan*np.ones((nodes, code_dim), dtype=np.float32)\n",
    "\n",
    "\t#Get instance for each node\n",
    "\tnode_data = data[np.arange(nodes), year]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\tfor batch in range(math.ceil(nodes/batch_size)):\n",
    "\n",
    "\t\t# GET BATCH DATA FOR FORWARD MODEL\n",
    "\t\tbatch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_dynamic_input = batch_data[:, :, dynamic_channels].to(device)\n",
    "\t\tbatch_static_input = torch.from_numpy(dataset_code[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_label = batch_data[:, :, output_channels].to(device)\n",
    "\t\t# print(batch_dynamic_input.shape, batch_static_input.shape, batch_label.shape)\n",
    "\n",
    "\t\t# GET FORWARD OUTPUT\n",
    "\t\tbatch_label_pred = forward_model(x_dynamic=batch_dynamic_input.to(device), x_static=batch_static_input.to(device))\n",
    "\t\t# print(batch_label_pred.shape)\n",
    "\n",
    "\t\t# STORE FORWARD OUTPUT\n",
    "\t\tdataset_true[batch*batch_size:(batch+1)*batch_size, year] = batch_label.detach().cpu().numpy()\n",
    "\t\tdataset_pred[batch*batch_size:(batch+1)*batch_size, year] = batch_label_pred.detach().cpu().numpy()\n",
    "\n",
    "\t\t# GET BATCH DATA FOR INVERSE MODEL\n",
    "\t\tbatch_input = batch_data[:,:,dynamic_channels+output_channels]\n",
    "\t\tbatch_static = batch_data[:,0,static_channels]\n",
    "\t\t# print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "\t\t# GET INVERSE OUTPUT\n",
    "\t\tbatch_code_vec, batch_mu, batch_std, batch_static_pred, batch_input_pred = inverse_model(x=batch_input)\n",
    "\t\t# print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "\t\t# STORE INVERSE OUTPUT\n",
    "\t\tyear_code[batch*batch_size:(batch+1)*batch_size] = batch_code_vec.detach().cpu().numpy()\n",
    "\n",
    "\tdataset_code = np.nanmean(np.concatenate((np.expand_dims(dataset_code, axis=1), np.expand_dims(year_code, axis=1)), axis=1), axis=1)\n",
    "\n",
    "dataset_true = (dataset_true*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "dataset_pred = (dataset_pred*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "dataset_true = UTILS.unstride_array(dataset_true)\n",
    "dataset_pred = UTILS.unstride_array(dataset_pred)\n",
    "dataset_true = dataset_true[:, stride:]\n",
    "dataset_pred = dataset_pred[:, stride:]\n",
    "\n",
    "per_sample_RMSE = UTILS.per_sample_RMSE(dataset_true, dataset_pred, unknown)\n",
    "_, per_node_RMSE = UTILS.per_node_RMSE(dataset_true, dataset_pred, unknown)\n",
    "per_sample_R2 = UTILS.per_sample_R2(dataset_true, dataset_pred, unknown)\n",
    "_, per_node_R2 = UTILS.per_node_R2(dataset_true, dataset_pred, unknown)\n",
    "print(\"Per Sample RMSE:{:.4f}\\tPer Node RMSE:{:.4f}\\tPer Sample R2:{:.4f}\\tPer Node R2:{:.4f}\".format(per_sample_RMSE, per_node_RMSE, per_sample_R2, per_node_R2))\n",
    "np.save(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"true\")), dataset_true)\n",
    "np.save(os.path.join(RESULT_DIR, \"{}_{}_{}_{}\".format(file, index, model_name, \"yearly_averaged_II\")), dataset_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUT DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file, index = \"strided_train\", \"test_index\"\n",
    "dataset = load_dataset(file)\n",
    "data = get_data(dataset, index)\n",
    "nodes, years, window, channels = data.shape\n",
    "# print(nodes, years, window, channels)\n",
    "\n",
    "dataset_code = np.nan*np.ones((nodes, years, code_dim), dtype=np.float32)\n",
    "for year in range(years):\n",
    "\n",
    "\t#Get instances for each node\n",
    "\tnode_data = data[np.arange(nodes), year]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\t# Remove pairs where node have unknown in streamflow\n",
    "\tkeep_idx = np.zeros((node_data.shape[0], 1)).astype(bool)\n",
    "\tkeep_idx[:,0] = (node_data[:,:,-1]!=unknown).all(axis=1)\n",
    "\tkeep_idx = keep_idx.all(axis=1)\n",
    "\tnode_data = node_data[keep_idx]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\tfor batch in range(math.ceil(node_data.shape[0]/batch_size)):\n",
    "\n",
    "\t\t# GET BATCH DATA FOR INVERSE MODEL\n",
    "\t\tbatch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_input = batch_data[:,:,dynamic_channels+output_channels]\n",
    "\t\tbatch_static = batch_data[:,0,static_channels]\n",
    "\t\t# print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "\t\t# GET INVERSE OUTPUT\n",
    "\t\tbatch_code_vec, batch_mu, batch_std, batch_static_pred, batch_input_pred = inverse_model(x=batch_input)\n",
    "\t\t# print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "\t\t# STORE OUTPUT\n",
    "\t\tdataset_code[batch*batch_size:(batch+1)*batch_size, year] = batch_code_vec.detach().cpu().numpy()\n",
    "dataset_code = np.nanmean(dataset_code, axis=1)\n",
    "\n",
    "file, index = \"strided_test\", \"test_index\"\n",
    "dataset = load_dataset(file)\n",
    "data = get_data(dataset, index)\n",
    "nodes, years, window, channels = data.shape\n",
    "# print(nodes, years, window, channels)\n",
    "\n",
    "dataset_true = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "dataset_pred = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "for year in range(years):\n",
    "\n",
    "\t# COLLECT EMBEDDINGS\n",
    "\tyear_code = np.nan*np.ones((nodes, code_dim), dtype=np.float32)\n",
    "\n",
    "\t#Get instance for each node\n",
    "\tnode_data = data[np.arange(nodes), year]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\tfor batch in range(math.ceil(nodes/batch_size)):\n",
    "\n",
    "\t\t# GET BATCH DATA FOR FORWARD MODEL\n",
    "\t\tbatch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_dynamic_input = batch_data[:, :, dynamic_channels].to(device)\n",
    "\t\tbatch_static_input = torch.from_numpy(dataset_code[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_label = batch_data[:, :, output_channels].to(device)\n",
    "\t\t# print(batch_dynamic_input.shape, batch_static_input.shape, batch_label.shape)\n",
    "\n",
    "\t\t# GET FORWARD OUTPUT\n",
    "\t\tbatch_label_pred = forward_model(x_dynamic=batch_dynamic_input.to(device), x_static=batch_static_input.to(device))\n",
    "\t\t# print(batch_label_pred.shape)\n",
    "\n",
    "\t\t# STORE FORWARD OUTPUT\n",
    "\t\tdataset_true[batch*batch_size:(batch+1)*batch_size, year] = batch_label.detach().cpu().numpy()\n",
    "\t\tdataset_pred[batch*batch_size:(batch+1)*batch_size, year] = batch_label_pred.detach().cpu().numpy()\n",
    "\n",
    "\t\t# GET BATCH DATA FOR INVERSE MODEL\n",
    "\t\tbatch_input = batch_data[:,:,dynamic_channels+output_channels]\n",
    "\t\tbatch_static = batch_data[:,0,static_channels]\n",
    "\t\t# print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "\t\t# GET INVERSE OUTPUT\n",
    "\t\tbatch_code_vec, batch_mu, batch_std, batch_static_pred, batch_input_pred = inverse_model(x=batch_input)\n",
    "\t\t# print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "\t\t# STORE INVERSE OUTPUT\n",
    "\t\tyear_code[batch*batch_size:(batch+1)*batch_size] = batch_code_vec.detach().cpu().numpy()\n",
    "\n",
    "\tdataset_code = np.nanmean(np.concatenate((np.expand_dims(dataset_code, axis=1), np.expand_dims(year_code, axis=1)), axis=1), axis=1)\n",
    "\n",
    "dataset_true = (dataset_true*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "dataset_pred = (dataset_pred*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "dataset_true = UTILS.unstride_array(dataset_true)\n",
    "dataset_pred = UTILS.unstride_array(dataset_pred)\n",
    "dataset_true = dataset_true[:, stride:]\n",
    "dataset_pred = dataset_pred[:, stride:]\n",
    "\n",
    "per_sample_RMSE = UTILS.per_sample_RMSE(dataset_true, dataset_pred, unknown)\n",
    "_, per_node_RMSE = UTILS.per_node_RMSE(dataset_true, dataset_pred, unknown)\n",
    "per_sample_R2 = UTILS.per_sample_R2(dataset_true, dataset_pred, unknown)\n",
    "_, per_node_R2 = UTILS.per_node_R2(dataset_true, dataset_pred, unknown)\n",
    "print(\"Per Sample RMSE:{:.4f}\\tPer Node RMSE:{:.4f}\\tPer Sample R2:{:.4f}\\tPer Node R2:{:.4f}\".format(per_sample_RMSE, per_node_RMSE, per_sample_R2, per_node_R2))\n",
    "np.save(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"true\")), dataset_true)\n",
    "np.save(os.path.join(RESULT_DIR, \"{}_{}_{}_{}\".format(file, index, model_name, \"yearly_averaged_II\")), dataset_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST MODEL (YEARLY EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IN DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file, index = \"strided_train\", \"train_index\"\n",
    "dataset = load_dataset(file)\n",
    "data = get_data(dataset, index)\n",
    "nodes, years, window, channels = data.shape\n",
    "# print(nodes, years, window, channels)\n",
    "\n",
    "dataset_code = np.nan*np.ones((nodes, years, code_dim), dtype=np.float32)\n",
    "for year in range(years):\n",
    "\n",
    "\t#Get instances for each node\n",
    "\tnode_data = data[np.arange(nodes), year]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\t# Remove pairs where node have unknown in streamflow\n",
    "\tkeep_idx = np.zeros((node_data.shape[0], 1)).astype(bool)\n",
    "\tkeep_idx[:,0] = (node_data[:,:,-1]!=unknown).all(axis=1)\n",
    "\tkeep_idx = keep_idx.all(axis=1)\n",
    "\tnode_data = node_data[keep_idx]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\tfor batch in range(math.ceil(node_data.shape[0]/batch_size)):\n",
    "\n",
    "\t\t# GET BATCH DATA FOR INVERSE MODEL\n",
    "\t\tbatch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_input = batch_data[:,:,dynamic_channels+output_channels]\n",
    "\t\tbatch_static = batch_data[:,0,static_channels]\n",
    "\t\t# print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "\t\t# GET INVERSE OUTPUT\n",
    "\t\tbatch_code_vec, batch_mu, batch_std, batch_static_pred, batch_input_pred = inverse_model(x=batch_input)\n",
    "\t\t# print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "\t\t# STORE OUTPUT\n",
    "\t\tdataset_code[batch*batch_size:(batch+1)*batch_size, year] = batch_code_vec.detach().cpu().numpy()\n",
    "dataset_code = np.nanmean(dataset_code, axis=1)\n",
    "\n",
    "file, index = \"strided_test\", \"train_index\"\n",
    "dataset = load_dataset(file)\n",
    "data = get_data(dataset, index)\n",
    "nodes, years, window, channels = data.shape\n",
    "# print(nodes, years, window, channels)\n",
    "\n",
    "dataset_true = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "dataset_pred = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "for year in range(years):\n",
    "\n",
    "\t# COLLECT EMBEDDINGS\n",
    "\tyear_code = np.nan*np.ones((nodes, code_dim), dtype=np.float32)\n",
    "\n",
    "\t#Get instance for each node\n",
    "\tnode_data = data[np.arange(nodes), year]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\tfor batch in range(math.ceil(nodes/batch_size)):\n",
    "\n",
    "\t\t# GET BATCH DATA FOR FORWARD MODEL\n",
    "\t\tbatch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_dynamic_input = batch_data[:, :, dynamic_channels].to(device)\n",
    "\t\tbatch_static_input = torch.from_numpy(dataset_code[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_label = batch_data[:, :, output_channels].to(device)\n",
    "\t\t# print(batch_dynamic_input.shape, batch_static_input.shape, batch_label.shape)\n",
    "\n",
    "\t\t# GET FORWARD OUTPUT\n",
    "\t\tbatch_label_pred = forward_model(x_dynamic=batch_dynamic_input.to(device), x_static=batch_static_input.to(device))\n",
    "\t\t# print(batch_label_pred.shape)\n",
    "\n",
    "\t\t# STORE FORWARD OUTPUT\n",
    "\t\tdataset_true[batch*batch_size:(batch+1)*batch_size, year] = batch_label.detach().cpu().numpy()\n",
    "\t\tdataset_pred[batch*batch_size:(batch+1)*batch_size, year] = batch_label_pred.detach().cpu().numpy()\n",
    "\n",
    "\t\t# GET BATCH DATA FOR INVERSE MODEL\n",
    "\t\tbatch_input = batch_data[:,:,dynamic_channels+output_channels]\n",
    "\t\tbatch_static = batch_data[:,0,static_channels]\n",
    "\t\t# print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "\t\t# GET INVERSE OUTPUT\n",
    "\t\tbatch_code_vec, batch_mu, batch_std, batch_static_pred, batch_input_pred = inverse_model(x=batch_input)\n",
    "\t\t# print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "\t\t# STORE INVERSE OUTPUT\n",
    "\t\tyear_code[batch*batch_size:(batch+1)*batch_size] = batch_code_vec.detach().cpu().numpy()\n",
    "\n",
    "\tdataset_code = year_code\n",
    "\n",
    "dataset_true = (dataset_true*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "dataset_pred = (dataset_pred*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "dataset_true = UTILS.unstride_array(dataset_true)\n",
    "dataset_pred = UTILS.unstride_array(dataset_pred)\n",
    "dataset_true = dataset_true[:, stride:]\n",
    "dataset_pred = dataset_pred[:, stride:]\n",
    "\n",
    "per_sample_RMSE = UTILS.per_sample_RMSE(dataset_true, dataset_pred, unknown)\n",
    "_, per_node_RMSE = UTILS.per_node_RMSE(dataset_true, dataset_pred, unknown)\n",
    "per_sample_R2 = UTILS.per_sample_R2(dataset_true, dataset_pred, unknown)\n",
    "_, per_node_R2 = UTILS.per_node_R2(dataset_true, dataset_pred, unknown)\n",
    "print(\"Per Sample RMSE:{:.4f}\\tPer Node RMSE:{:.4f}\\tPer Sample R2:{:.4f}\\tPer Node R2:{:.4f}\".format(per_sample_RMSE, per_node_RMSE, per_sample_R2, per_node_R2))\n",
    "np.save(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"true\")), dataset_true)\n",
    "np.save(os.path.join(RESULT_DIR, \"{}_{}_{}_{}\".format(file, index, model_name, \"yearly\")), dataset_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUT DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file, index = \"strided_train\", \"test_index\"\n",
    "dataset = load_dataset(file)\n",
    "data = get_data(dataset, index)\n",
    "nodes, years, window, channels = data.shape\n",
    "# print(nodes, years, window, channels)\n",
    "\n",
    "dataset_code = np.nan*np.ones((nodes, years, code_dim), dtype=np.float32)\n",
    "for year in range(years):\n",
    "\n",
    "\t#Get instances for each node\n",
    "\tnode_data = data[np.arange(nodes), year]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\t# Remove pairs where node have unknown in streamflow\n",
    "\tkeep_idx = np.zeros((node_data.shape[0], 1)).astype(bool)\n",
    "\tkeep_idx[:,0] = (node_data[:,:,-1]!=unknown).all(axis=1)\n",
    "\tkeep_idx = keep_idx.all(axis=1)\n",
    "\tnode_data = node_data[keep_idx]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\tfor batch in range(math.ceil(node_data.shape[0]/batch_size)):\n",
    "\n",
    "\t\t# GET BATCH DATA FOR INVERSE MODEL\n",
    "\t\tbatch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_input = batch_data[:,:,dynamic_channels+output_channels]\n",
    "\t\tbatch_static = batch_data[:,0,static_channels]\n",
    "\t\t# print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "\t\t# GET INVERSE OUTPUT\n",
    "\t\tbatch_code_vec, batch_mu, batch_std, batch_static_pred, batch_input_pred = inverse_model(x=batch_input)\n",
    "\t\t# print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "\t\t# STORE OUTPUT\n",
    "\t\tdataset_code[batch*batch_size:(batch+1)*batch_size, year] = batch_code_vec.detach().cpu().numpy()\n",
    "dataset_code = np.nanmean(dataset_code, axis=1)\n",
    "\n",
    "file, index = \"strided_test\", \"test_index\"\n",
    "dataset = load_dataset(file)\n",
    "data = get_data(dataset, index)\n",
    "nodes, years, window, channels = data.shape\n",
    "# print(nodes, years, window, channels)\n",
    "\n",
    "dataset_true = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "dataset_pred = unknown*np.ones((nodes, years, window, len(output_channels)), dtype=np.float32)\n",
    "for year in range(years):\n",
    "\n",
    "\t# COLLECT EMBEDDINGS\n",
    "\tyear_code = np.nan*np.ones((nodes, code_dim), dtype=np.float32)\n",
    "\n",
    "\t#Get instance for each node\n",
    "\tnode_data = data[np.arange(nodes), year]\n",
    "\t# print(node_data.shape)\n",
    "\n",
    "\tfor batch in range(math.ceil(nodes/batch_size)):\n",
    "\n",
    "\t\t# GET BATCH DATA FOR FORWARD MODEL\n",
    "\t\tbatch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_dynamic_input = batch_data[:, :, dynamic_channels].to(device)\n",
    "\t\tbatch_static_input = torch.from_numpy(dataset_code[batch*batch_size:(batch+1)*batch_size]).to(device)\n",
    "\t\tbatch_label = batch_data[:, :, output_channels].to(device)\n",
    "\t\t# print(batch_dynamic_input.shape, batch_static_input.shape, batch_label.shape)\n",
    "\n",
    "\t\t# GET FORWARD OUTPUT\n",
    "\t\tbatch_label_pred = forward_model(x_dynamic=batch_dynamic_input.to(device), x_static=batch_static_input.to(device))\n",
    "\t\t# print(batch_label_pred.shape)\n",
    "\n",
    "\t\t# STORE FORWARD OUTPUT\n",
    "\t\tdataset_true[batch*batch_size:(batch+1)*batch_size, year] = batch_label.detach().cpu().numpy()\n",
    "\t\tdataset_pred[batch*batch_size:(batch+1)*batch_size, year] = batch_label_pred.detach().cpu().numpy()\n",
    "\n",
    "\t\t# GET BATCH DATA FOR INVERSE MODEL\n",
    "\t\tbatch_input = batch_data[:,:,dynamic_channels+output_channels]\n",
    "\t\tbatch_static = batch_data[:,0,static_channels]\n",
    "\t\t# print(batch_input.shape, batch_static.shape)\n",
    "\n",
    "\t\t# GET INVERSE OUTPUT\n",
    "\t\tbatch_code_vec, batch_mu, batch_std, batch_static_pred, batch_input_pred = inverse_model(x=batch_input)\n",
    "\t\t# print(batch_code_vec.shape, batch_static_pred.shape, batch_input_pred.shape)\n",
    "\n",
    "\t\t# STORE INVERSE OUTPUT\n",
    "\t\tyear_code[batch*batch_size:(batch+1)*batch_size] = batch_code_vec.detach().cpu().numpy()\n",
    "\n",
    "\tdataset_code = year_code\n",
    "\n",
    "dataset_true = (dataset_true*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "dataset_pred = (dataset_pred*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels]\n",
    "dataset_true = UTILS.unstride_array(dataset_true)\n",
    "dataset_pred = UTILS.unstride_array(dataset_pred)\n",
    "dataset_true = dataset_true[:, stride:]\n",
    "dataset_pred = dataset_pred[:, stride:]\n",
    "\n",
    "per_sample_RMSE = UTILS.per_sample_RMSE(dataset_true, dataset_pred, unknown)\n",
    "_, per_node_RMSE = UTILS.per_node_RMSE(dataset_true, dataset_pred, unknown)\n",
    "per_sample_R2 = UTILS.per_sample_R2(dataset_true, dataset_pred, unknown)\n",
    "_, per_node_R2 = UTILS.per_node_R2(dataset_true, dataset_pred, unknown)\n",
    "print(\"Per Sample RMSE:{:.4f}\\tPer Node RMSE:{:.4f}\\tPer Sample R2:{:.4f}\\tPer Node R2:{:.4f}\".format(per_sample_RMSE, per_node_RMSE, per_sample_R2, per_node_R2))\n",
    "np.save(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"true\")), dataset_true)\n",
    "np.save(os.path.join(RESULT_DIR, \"{}_{}_{}_{}\".format(file, index, model_name, \"yearly\")), dataset_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
