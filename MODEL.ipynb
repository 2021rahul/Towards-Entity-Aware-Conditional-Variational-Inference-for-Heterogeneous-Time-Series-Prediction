{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a76fa824-0294-4ffa-b417-b4dcc6e74e27",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8ce801-9f4e-47c0-8676-c20610f0ec13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kumarv/ghosh128/anaconda3/envs/main/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from EALSTM import EALSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8c6374-fd57-4853-b4a6-32e45c50186d",
   "metadata": {},
   "source": [
    "# LOSS FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde04960-18f3-4fa5-8171-0dd5e3d9310a",
   "metadata": {},
   "source": [
    "## CONTRASTIVE LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56b899d3-b222-4465-81f1-2b93b7b238aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLRLoss(torch.nn.Module):\n",
    "\tdef __init__(self, temperature):\n",
    "\t\tsuper(SimCLRLoss, self).__init__()\n",
    "\t\tself.temperature = temperature\n",
    "\t\tself.criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\t\tself.similarity = torch.nn.CosineSimilarity(dim=2)\n",
    "\n",
    "\tdef mask_correlated_samples(self, batch_size):\n",
    "\t\tN = 2 * batch_size\n",
    "\t\tmask = torch.ones((N, N), dtype=bool)\n",
    "\t\tmask = mask.fill_diagonal_(0)\n",
    "\n",
    "\t\tfor i in range(batch_size):\n",
    "\t\t\tmask[i, batch_size + i] = 0\n",
    "\t\t\tmask[batch_size + i, i] = 0\n",
    "\t\treturn mask\n",
    "\n",
    "\tdef forward(self, z):\n",
    "\n",
    "\t\tz = torch.nn.functional.normalize(z, p=2.0, dim=1)\n",
    "\n",
    "\t\tN = z.shape[0]\n",
    "\t\tbatch_size = N//2\n",
    "\n",
    "\t\tsim = self.similarity(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "\n",
    "\t\tsim_i_j = torch.diag(sim, batch_size)\n",
    "\t\tsim_j_i = torch.diag(sim, -batch_size)\n",
    "\n",
    "\t\tpositive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "\t\tmask = self.mask_correlated_samples(batch_size)\n",
    "\t\tnegative_samples = sim[mask].reshape(N, -1)\n",
    "\n",
    "\t\t#SIMCLR\n",
    "\t\tlabels = torch.from_numpy(np.array([0]*N)).reshape(-1).to(positive_samples.device).long()\n",
    "\n",
    "\t\tlogits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "\t\tloss = self.criterion(logits, labels)\n",
    "\t\tloss /= N\n",
    "\n",
    "\t\treturn loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace922e6-a458-4732-9813-5e250f81b583",
   "metadata": {},
   "source": [
    "## KL LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca919c9-1aea-4129-bb24-ee0029f617d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLLoss(torch.nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(KLLoss, self).__init__()\n",
    "\n",
    "\tdef forward(self, z, mu, std):\n",
    "\t\t# 1. define the first two probabilities (in this case Normal for both)\n",
    "\t\tp = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "\t\tq = torch.distributions.Normal(mu, std)\n",
    "\n",
    "\t\t# 2. get the probabilities from the equation\n",
    "\t\tlog_qzx = q.log_prob(z)\n",
    "\t\tlog_pz = p.log_prob(z)\n",
    "\n",
    "\t\t# loss\n",
    "\t\tloss = (log_qzx - log_pz)\n",
    "\t\tloss = torch.mean(torch.sum(loss, dim=1), dim = 0)\n",
    "\t\treturn loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1742f6-1e91-480e-98f0-39f84874f292",
   "metadata": {},
   "source": [
    "# FORWARD MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32ed8b-badf-458c-a18e-7eb6d5003240",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5462112a-9d3e-4edd-ae61-e91785991891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(torch.nn.Module):\n",
    "\n",
    "\tdef __init__(self, input_channels, code_dim, output_channels):\n",
    "\t\tsuper(lstm,self).__init__()\n",
    "\n",
    "\t\t# PARAMETERS\n",
    "\t\tself.input_channels = input_channels\n",
    "\t\tself.code_dim = code_dim\n",
    "\t\tself.output_channels = output_channels\n",
    "\n",
    "\t\t# LAYERS\n",
    "\t\tself.encoder = torch.nn.LSTM(input_size=self.input_channels, hidden_size=self.code_dim, batch_first=True)\n",
    "\t\tself.out = torch.nn.Linear(in_features=self.code_dim, out_features=self.output_channels)\n",
    "\n",
    "\t\t# INITIALIZATION\n",
    "\t\tfor m in self.modules():\n",
    "\t\t\tif isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n",
    "\t\t\t\ttorch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "\tdef forward(self, x_dynamic):\n",
    "\n",
    "\t\t# GET SHAPES\n",
    "\t\tbatch, window, _ = x_dynamic.shape\n",
    "\n",
    "\t\t# OPERATIONS\n",
    "\t\tx_encoder, _ = self.encoder(x_dynamic)\n",
    "\t\tout = self.out(x_encoder)\n",
    "\t\tout = out.view(batch, window, self.output_channels)\n",
    "\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632402ef-d25b-492f-a8f8-bd313b1d09cc",
   "metadata": {},
   "source": [
    "## EALSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd368fb7-b3ba-4e2d-880c-95d61d60221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ealstm(torch.nn.Module):\n",
    "\n",
    "\tdef __init__(self, input_dynamic_channels, input_static_channels, code_dim, output_channels):\n",
    "\t\tsuper(ealstm,self).__init__()\n",
    "\n",
    "\t\t# PARAMETERS\n",
    "\t\tself.input_dynamic_channels = input_dynamic_channels\n",
    "\t\tself.input_static_channels = input_static_channels\n",
    "\t\tself.code_dim = code_dim\n",
    "\t\tself.output_channels = output_channels\n",
    "\n",
    "\t\t# LAYERS\n",
    "\t\tself.encoder = EALSTM(input_size_dyn=self.input_dynamic_channels, input_size_stat=self.input_static_channels, hidden_size=self.code_dim, batch_first=True)\n",
    "\t\tself.out = torch.nn.Linear(in_features=self.code_dim, out_features=self.output_channels)\n",
    "\n",
    "\t\t# INITIALIZATION\n",
    "\t\tfor m in self.modules():\n",
    "\t\t\tif isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n",
    "\t\t\t\ttorch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "\tdef forward(self, x_dynamic, x_static):\n",
    "\n",
    "\t\t# GET SHAPES\n",
    "\t\tbatch, window, _ = x_dynamic.shape\n",
    "\n",
    "\t\t# OPERATIONS\n",
    "\t\tx_encoder, _ = self.encoder(x_d=x_dynamic, x_s=x_static)\n",
    "\t\tout = self.out(x_encoder)\n",
    "\t\tout = out.view(batch, window, self.output_channels)\n",
    "\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2ecfed-ae66-4864-a2e7-217ca40599b7",
   "metadata": {},
   "source": [
    "## CTLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cfd524a-aa18-4d57-9588-dd2cef3f0ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ctlstm(torch.nn.Module):\n",
    "\n",
    "\tdef __init__(self, input_dynamic_channels, input_static_channels, code_dim, output_channels):\n",
    "\t\tsuper(ctlstm,self).__init__()\n",
    "\n",
    "\t\t# PARAMETERS\n",
    "\t\tself.input_dynamic_channels = input_dynamic_channels\n",
    "\t\tself.input_static_channels = input_static_channels\n",
    "\t\tself.code_dim = code_dim\n",
    "\t\tself.output_channels = output_channels\n",
    "\n",
    "\t\t# LAYERS\n",
    "\t\tself.encoder = torch.nn.LSTM(input_size=self.input_dynamic_channels+self.input_static_channels, hidden_size=self.code_dim, batch_first=True)\n",
    "\t\tself.out = torch.nn.Linear(in_features=self.code_dim, out_features=self.output_channels)\n",
    "\n",
    "\t\t# INITIALIZATION\n",
    "\t\tfor m in self.modules():\n",
    "\t\t\tif isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n",
    "\t\t\t\ttorch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "\tdef forward(self, x_dynamic, x_static):\n",
    "\n",
    "\t\t# GET SHAPES\n",
    "\t\tbatch, window, _ = x_dynamic.shape\n",
    "\n",
    "\t\t# OPERATIONS\n",
    "\t\tx = torch.cat((x_dynamic, x_static), dim=-1)\n",
    "\t\tx_encoder, _ = self.encoder(x)\n",
    "\t\tout = self.out(x_encoder)\n",
    "\t\tout = out.view(batch, window, self.output_channels)\n",
    "\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3eb64-5d27-46da-9120-ee61fa01de2b",
   "metadata": {},
   "source": [
    "# INVERSE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9e8c09-ddbb-4dd1-9863-f9e2972883ff",
   "metadata": {},
   "source": [
    "## AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6f3b8ef-c068-4115-b7fe-4c0af7cbf6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ae(torch.nn.Module):\n",
    "\tdef __init__(self, input_channels, code_dim, output_channels, device):\n",
    "\t\tsuper(ae,self).__init__()\n",
    "\n",
    "\t\t# PARAMETERS\n",
    "\t\tself.input_channels = input_channels\n",
    "\t\tself.code_dim = code_dim\n",
    "\t\tself.output_channels = output_channels\n",
    "\t\tself.device = device\n",
    "\n",
    "\t\t# LAYERS\n",
    "\t\tself.instance_encoder = torch.nn.Sequential(\n",
    "\t\t\ttorch.nn.Linear(in_features=self.input_channels, out_features=self.code_dim),\n",
    "\t\t\ttorch.nn.BatchNorm1d(self.code_dim),\n",
    "\t\t\ttorch.nn.LeakyReLU(0.2)\n",
    "\t\t)\n",
    "\t\tself.temporal_encoder = torch.nn.LSTM(input_size=self.code_dim, hidden_size=self.code_dim, bidirectional=True, batch_first=True)\t# AE\n",
    "\t\ttorch.nn.BatchNorm1d(self.code_dim)\n",
    "\t\tself.temporal_decoder = torch.nn.LSTM(input_size=self.code_dim, hidden_size=self.code_dim, batch_first=True)\t\t\t\t\t\t# AE\n",
    "\t\tself.instance_decoder = torch.nn.Linear(in_features=self.code_dim, out_features=self.input_channels)\t\t\t\t\t\t\t\t# AE\n",
    "\t\tself.static_out = torch.nn.Linear(in_features=self.code_dim, out_features=self.output_channels)\n",
    "\n",
    "\t\t# INITIALIZATION\n",
    "\t\tfor m in self.modules():\n",
    "\t\t\tif isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n",
    "\t\t\t\ttorch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\n",
    "\t\t# GET SHAPES\n",
    "\t\tbatch, window, _ = x.shape\n",
    "\n",
    "\t\t# OPERATIONS\n",
    "\n",
    "\t\tx_encoder = self.instance_encoder(x.view(-1, self.input_channels)).view(batch, window, -1)\t\t\t\t\t\t\t\t# ENCODE\n",
    "\t\t_, x_encoder = self.temporal_encoder(x_encoder)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# ENCODE\n",
    "\t\tcode_vec = torch.sum(x_encoder[0], dim=0)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# ENCODE\n",
    "\n",
    "\t\tstatic_out = self.static_out(code_vec)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# STATIC DECODE\n",
    "\n",
    "\t\tout = torch.zeros(batch, window, self.input_channels).to(self.device)\t\t\t\t\t\t\t\t\t\t\t\t\t# DECODE\n",
    "\t\tinput = torch.unsqueeze(torch.zeros_like(code_vec), dim=1)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# DECODE\n",
    "\t\th = (torch.unsqueeze(torch.sum(x_encoder[0], dim=0), dim=0), torch.unsqueeze(torch.sum(x_encoder[1], dim=0), dim=0))\t# DECODE\n",
    "\t\tfor step in range(window):\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# DECODE\n",
    "\t\t\tinput, h = self.temporal_decoder(input, h)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# DECODE\n",
    "\t\t\tout[:,step] = self.instance_decoder(input.squeeze())\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# DECODE\n",
    "\n",
    "\t\treturn code_vec, static_out, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d3d1e-3351-45ed-9173-3355328942f9",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ab50072-f36b-41c1-85b6-25661ccf9d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vae(torch.nn.Module):\n",
    "\tdef __init__(self, input_channels, code_dim, output_channels, device):\n",
    "\t\tsuper(vae,self).__init__()\n",
    "\n",
    "\t\t# PARAMETERS\n",
    "\t\tself.input_channels = input_channels\n",
    "\t\tself.code_dim = code_dim\n",
    "\t\tself.output_channels = output_channels\n",
    "\t\tself.device = device\n",
    "\n",
    "\t\t# LAYERS\n",
    "\t\tself.instance_encoder = torch.nn.Sequential(\n",
    "\t\t\ttorch.nn.Linear(in_features=self.input_channels, out_features=self.code_dim),\n",
    "\t\t\ttorch.nn.BatchNorm1d(self.code_dim),\n",
    "\t\t\ttorch.nn.LeakyReLU(0.2)\n",
    "\t\t)\n",
    "\t\tself.temporal_encoder = torch.nn.LSTM(input_size=self.code_dim, hidden_size=self.code_dim, bidirectional=True, batch_first=True)\t# VAE\n",
    "\t\tself.mu = torch.nn.Linear(self.code_dim, self.code_dim)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# VAE\n",
    "\t\tself.log_var = torch.nn.Linear(self.code_dim, self.code_dim)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# VAE\n",
    "\t\tself.temporal_decoder = torch.nn.LSTM(input_size=self.code_dim, hidden_size=self.code_dim, batch_first=True)\t\t\t\t\t\t# VAE\n",
    "\t\tself.instance_decoder = torch.nn.Linear(in_features=self.code_dim, out_features=self.input_channels)\t\t\t\t\t\t\t\t# VAE\n",
    "\t\tself.static_out = torch.nn.Linear(in_features=self.code_dim, out_features=self.output_channels)\n",
    "\n",
    "\t\t# INITIALIZATION\n",
    "\t\tfor m in self.modules():\n",
    "\t\t\tif isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n",
    "\t\t\t\ttorch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\n",
    "\t\t# GET SHAPES\n",
    "\t\tbatch, window, _ = x.shape\n",
    "\n",
    "\t\t# OPERATIONS\n",
    "\n",
    "\t\tx_encoder = self.instance_encoder(x.view(-1, self.input_channels)).view(batch, window, -1)\t# ENCODE\n",
    "\t\t_, x_encoder = self.temporal_encoder(x_encoder)\t\t\t\t\t\t\t\t\t\t\t\t# ENCODE\n",
    "\t\tcode_vec = torch.sum(x_encoder[0], dim=0)\t\t\t\t\t\t\t\t\t\t\t\t\t# ENCODE\n",
    "\n",
    "\t\tmu, log_var = self.mu(code_vec), self.log_var(code_vec)\t\t\t\t\t\t\t\t\t\t# SAMPLE Z\n",
    "\t\tstd = torch.exp(log_var/2)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# SAMPLE Z\n",
    "\t\tz = mu + std * torch.randn_like(std)\t\t\t\t\t\t\t\t\t\t\t\t\t\t# SAMPLE Z\n",
    "\n",
    "\t\tstatic_out = self.static_out(z)\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# STATIC DECODE\n",
    "\n",
    "\t\tout = torch.zeros(batch, window, self.input_channels).to(self.device)\t\t\t\t\t\t# DECODE\n",
    "\t\tinput = torch.unsqueeze(torch.zeros_like(z), dim=1)\t\t\t\t\t\t\t\t\t\t\t# DECODE\n",
    "\t\th = (torch.unsqueeze(z, dim=0), torch.unsqueeze(torch.sum(x_encoder[1], dim=0), dim=0))\t\t# DECODE\n",
    "\t\tfor step in range(window):\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# DECODE\n",
    "\t\t\tinput, h = self.temporal_decoder(input, h)\t\t\t\t\t\t\t\t\t\t\t\t# DECODE\n",
    "\t\t\tout[:,step] = self.instance_decoder(input.squeeze())\t\t\t\t\t\t\t\t\t# DECODE\n",
    "\n",
    "\t\treturn z, mu, std, static_out, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b39abee-296b-4091-b9ff-2f4bebb1c70d",
   "metadata": {},
   "source": [
    "# TEST MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8cfac56-c8ad-4924-8300-44b5ef8a4c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 365, 32]) DATA\n",
      "torch.Size([10, 365, 5]) torch.Size([10, 365, 1]) #:69249 lstm\n",
      "torch.Size([10, 365, 32]) torch.Size([10, 365, 1]) #:55169 ealstm\n",
      "torch.Size([10, 365, 32]) torch.Size([10, 365, 1]) #:83073 ctlstm\n",
      "torch.Size([10, 365, 32]) torch.Size([10, 128]) torch.Size([10, 27]) torch.Size([10, 365, 5]) #:401440 ae\n",
      "torch.Size([10, 365, 32]) torch.Size([10, 128]) torch.Size([10, 128]) torch.Size([10, 128]) torch.Size([10, 27]) torch.Size([10, 365, 5]) #:434464 vae\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\tbatch = 10\n",
    "\twindow = 365\n",
    "\tchannels = list(range(33))\n",
    "\tstatic_channels = channels[:27]\n",
    "\tdynamic_channels = channels[27:32]\n",
    "\toutput_channels = [channels[-1]]\n",
    "\tdata = torch.randn(batch, window, len(static_channels)+len(dynamic_channels))\n",
    "\tdata_dynamic = data[:,:,dynamic_channels]\n",
    "\tdata_static = data[:,:,static_channels]\n",
    "\tprint(data.shape, \"DATA\")\n",
    "\n",
    "\tcode_dim = 128\n",
    "\tdevice = torch.device(\"cuda\")\n",
    "\n",
    "\tarchitecture = \"lstm\"\n",
    "\tmodel = globals()[architecture](input_channels=len(dynamic_channels), code_dim=code_dim, output_channels=len(output_channels))\n",
    "\tmodel = model.to(device)\n",
    "\tpytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\tout = model(data_dynamic.to(device))\n",
    "\tprint(data[:, :, dynamic_channels].shape, out.shape, \"#:{}\".format(pytorch_total_params), architecture)\n",
    "\n",
    "\tarchitecture = \"ealstm\"\n",
    "\tmodel = globals()[architecture](input_dynamic_channels=len(dynamic_channels), input_static_channels=len(static_channels), code_dim=code_dim, output_channels=len(output_channels))\n",
    "\tmodel = model.to(device)\n",
    "\tpytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\tout = model(data_dynamic.to(device), data_static[:,0].to(device))\n",
    "\tprint(data.shape, out.shape, \"#:{}\".format(pytorch_total_params), architecture)\n",
    "\n",
    "\tarchitecture = \"ctlstm\"\n",
    "\tmodel = globals()[architecture](input_dynamic_channels=len(dynamic_channels), input_static_channels=len(static_channels), code_dim=code_dim, output_channels=len(output_channels))\n",
    "\tmodel = model.to(device)\n",
    "\tpytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\tout = model(data_dynamic.to(device), data_static.to(device))\n",
    "\tprint(data.shape, out.shape, \"#:{}\".format(pytorch_total_params), architecture)\n",
    "\n",
    "\tarchitecture = \"ae\"\n",
    "\tmodel = globals()[architecture](input_channels=len(dynamic_channels), code_dim=code_dim, output_channels=len(static_channels), device=device)\n",
    "\tmodel = model.to(device)\n",
    "\tpytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\tcode_vec, static_out, out = model(data[:, :, dynamic_channels].to(device))\n",
    "\tprint(data.shape, code_vec.shape, static_out.shape, out.shape, \"#:{}\".format(pytorch_total_params), architecture)\n",
    "\n",
    "\tarchitecture = \"vae\"\n",
    "\tmodel = globals()[architecture](input_channels=len(dynamic_channels), code_dim=code_dim, output_channels=len(static_channels), device=device)\n",
    "\tmodel = model.to(device)\n",
    "\tpytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\tcode_vec, mu, std, static_out, out = model(data[:, :, dynamic_channels].to(device))\n",
    "\tprint(data.shape, code_vec.shape, mu.shape, std.shape, static_out.shape, out.shape, \"#:{}\".format(pytorch_total_params), architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6df53f-e41c-4966-af14-29aa1028da04",
   "metadata": {},
   "source": [
    "# COPY MODEL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da67273c-63c0-4743-b481-c0f63d3cb405",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "\tarchitecture = \"ae\"\n",
    "\tmodel1 = globals()[architecture](input_channels=len(dynamic_channels), code_dim=code_dim, output_channels=len(static_channels), device=device)\n",
    "\tmodel1 = model1.to(device)\n",
    "\n",
    "\tarchitecture = \"vae\"\n",
    "\tmodel2 = globals()[architecture](input_channels=len(dynamic_channels), code_dim=code_dim, output_channels=len(static_channels), device=device)\n",
    "\tmodel2 = model2.to(device)\n",
    "\n",
    "\tmodel2.load_state_dict(model1.state_dict(), strict=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
